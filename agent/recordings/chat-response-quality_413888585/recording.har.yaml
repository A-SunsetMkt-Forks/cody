log:
  _recordingName: chat-response-quality
  creator:
    comment: persister:fs
    name: Polly.JS
    version: 6.0.6
  entries:
    - _id: e07eccc8166c2864b4c21f2518d0ad3a
      _order: 0
      cache: {}
      request:
        bodySize: 5076
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-aa9bff29ca90bc426d6a621a2db70514-02af52de778f0d45-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: What does this
                  repo do??"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 268724
        content:
          mimeType: text/event-stream
          size: 268724
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, it seems that this repository is part of a larger project called \"zoekt\". The code deals with handling file matches and truncating or limiting the results based on certain search options.\n\nThe main functionality revolves around the `SortAndTruncateFiles` function, which sorts the file matches and then truncates them based on the provided search options using a `DisplayTruncator`.\n\nThe `DisplayTruncator` is a stateful function that enforces limits on the number of documents (files) and matches displayed. The `NewDisplayTruncator` function creates a `DisplayTruncator` instance based on the search options, which include `MaxDocDisplayCount` and `MaxMatchDisplayCount`.\n\nThe `limitMatches` function is responsible for limiting the number of matches (either chunk matches or line matches) based on the provided limit and a flag indicating whether to chunk matches or not.\n\nThe `limitChunkMatches` and `limitLineMatches` functions are helpers that truncate the chunk matches and line matches, respectively, based on the provided limit.\n\nOverall, this code seems to be part of a search engine or a tool that processes and displays search results, with the ability to limit the number of files and matches shown based on user-specified options.\n\nHowever, without broader context from the rest of the repository, it's difficult to determine the exact purpose or functionality of this project. Additional context about the overall project structure, dependencies, and usage would be helpful to fully understand the purpose of this repository.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:04:36 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:04:34.194Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: dcfe4cba81cd4adf2c7eb61143242e41
      _order: 0
      cache: {}
      request:
        bodySize: 32342
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-c38baebb016588f60410398d077a5804-1d6ee974e872f91f-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: describe my
                  code"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 159311
        content:
          mimeType: text/event-stream
          size: 159311
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, here's a high-level overview:\n\n1. `eval.go` contains the core logic for executing search queries and returning search results. It defines functions like `Search` and `List` that take a query and search options, and return the relevant search results or repository list.\n\n2. `limit.go` defines functions related to limiting and truncating search results based on various options, such as maximum match count, maximum document count, and whether to merge or separate overlapping matches.\n\n3. `README.md` provides an overview of Zoekt, a fast text search engine intended for source code. It includes instructions for indexing and searching code repositories, as well as some background information on the project.\n\n4. `vscode/src/external-services.ts` is a TypeScript file, likely related to integrating Zoekt with the Visual Studio Code editor.\n\nWith this high-level overview, I can provide a more detailed description if you have a specific question about the codebase. If you would like me to describe how the code works in more detail, please let me know, and I may request additional context from specific files or functions.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:04:46 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:04:42.783Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 01e59da36271c30cf33fdb2e9b47179e
      _order: 0
      cache: {}
      request:
        bodySize: 24006
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-358bbdbd23ab6cdff33b79ca25f153ea-779c7329a605d3cf-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: @zoekt
                  describe my code"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 175851
        content:
          mimeType: text/event-stream
          size: 175851
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, it appears that this codebase is related to a search engine or indexing system called \"zoekt\". The code seems to handle searching across multiple shards or partitions of data, with functionalities like streaming search, handling different search options, and scoring the search results.\n\nSome key points:\n\n1. The code deals with indexing and searching documents, handling features like file names, content, branches, and repositories.\n2. There are structs and functions for managing sharded searchers (`shardedSearcher`), handling search queries (`query.Q`), and building indexes (`IndexBuilder`).\n3. There are functions for performing regular searches (`Search`) and streaming searches (`StreamSearch`).\n4. The code includes benchmarking and testing utilities for the search functionality.\n5. There are implementations for handling different search options, such as document ranking, context lines, and scoring methods (e.g., BM25).\n6. The code seems to support features like handling public/private repositories, forks, and archived repositories.\n\nTo provide a more detailed overview and answer your specific question, I would need additional context about the overall architecture, purpose, and usage of this codebase.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:04:53 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:04:50.647Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: c004e80cffb73613c6ba99ddfa074185
      _order: 0
      cache: {}
      request:
        bodySize: 3036
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-aa62b677120e2e93d9225ec255cbf53d-91062ff222905975-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/aggregate.go:

                  ```go

                  var metricFinalAggregateSize = promauto.NewHistogramVec(prometheus.HistogramOpts{
                  	Name:    "zoekt_final_aggregate_size",
                  	Help:    "The number of file matches we aggregated before flushing",
                  	Buckets: prometheus.ExponentialBuckets(1, 2, 20),
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: is my code
                  base clean?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 416893
        content:
          mimeType: text/event-stream
          size: 416893
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, I can give you a high-level overview related to the code quality and cleanliness of the codebase.\n\nThe `shards/aggregate.go` file contains a Prometheus metric for tracking the number of file matches aggregated before flushing. This metric can be used to monitor the performance and efficiency of the search engine. However, without seeing the surrounding code, it's difficult to assess the overall code quality of this file or the larger codebase.\n\nThe `README.md` file provides instructions for downloading, indexing, and searching with Zoekt, a fast text search engine for source code. It includes examples for indexing directories, Git repositories, and Repo repositories. The README also explains how to run the web interface and interact with the JSON API.\n\nWhile the README is well-written and provides clear instructions, it does not directly reveal information about the code quality or cleanliness of the codebase itself.\n\nTo fully assess the cleanliness and quality of the codebase, I would need additional context and information, such as:\n\n1. The overall project structure and organization of the codebase.\n2. The coding style and conventions followed throughout the project.\n3. The presence and quality of test cases and code coverage.\n4. The use of best practices for error handling, logging, and monitoring.\n5. The adherence to coding principles such as SOLID, DRY, and KISS.\n6. The quality and maintainability of the documentation (beyond just the README).\n7. The complexity and readability of the core functionality and algorithms.\n8. The presence of technical debt, code smells, or areas in need of refactoring.\n\nWithout access to the complete codebase and a more in-depth analysis, it's difficult to provide a definitive assessment of the code cleanliness. Please let me know if you can provide additional context or specific areas of concern within the codebase.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:00 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:04:59.545Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 5c7c9acef1b60b3433541965ca6364f6
      _order: 0
      cache: {}
      request:
        bodySize: 10254
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-b7e735fc8dfb378dc41f41fc02b50e96-00ff677d5658d9f0-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Can you look
                  through the files and identify the conflicting packages that
                  may be causing this?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 233456
        content:
          mimeType: text/event-stream
          size: 233456
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, I can see that this is a part of the Zoekt codebase, which is a fast text search engine intended for use with source code. The `limit.go` file appears to contain functions related to limiting the number of search results and matches returned based on certain options.\n\nHowever, I do not have enough context from the provided files to identify any conflicting packages that may be causing issues. The `README.md` file provides an overview of the project and instructions for installation, indexing, and searching, but it does not mention anything about conflicting packages or issues with dependencies.\n\nTo fully answer the question about identifying conflicting packages, I would need additional context, such as:\n\n1. Information about the specific issue or error you are encountering related to conflicting packages.\n2. Access to the project's dependency management files (e.g., `go.mod`, `Gopkg.toml`, etc.) and the project's directory structure to understand the project's dependencies and package organization.\n3. Logs or error messages that might provide more details about the conflicting packages or dependencies.\n4. Information about the development environment and any recent changes made to the codebase or dependencies.\n\nWithout this additional context, it is difficult for me to pinpoint the cause of conflicting packages or provide a more specific answer based solely on the `README.md` and `limit.go` files provided.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:10 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:09.121Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 65dc3e87c050dec3cdfa34327397ec73
      _order: 0
      cache: {}
      request:
        bodySize: 32374
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-cb0e50b8531969a884797a3843edbb29-4c5ed2625c390618-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Mind taking a
                  second look at the file? @limit.go"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 38298
        content:
          mimeType: text/event-stream
          size: 38298
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, I can give a high-level overview for the `limit.go` file:\n\nThe `limit.go` file contains utility functions related to search and limiting results. The `SortAndTruncate` function is likely used to sort and truncate a list of file matches based on a search pattern and maximum number of results. \n\nFor the `@limit.go` file, I don't have enough context to fully explain what it is being used for. I need to see the surrounding application logic and how that function is being called/used.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:21 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:17.277Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: abf181fa88dfb758ffd2091b087a4bf6
      _order: 0
      cache: {}
      request:
        bodySize: 10202
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-76167df218579cb8f4477934c38d405f-cca91a02e055effe-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Why does this
                  project use the MIT license?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 135531
        content:
          mimeType: text/event-stream
          size: 135531
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, it appears that Zoekt is a fast text search engine intended for use with source code. The codebase contains functionality for indexing directories, Git repositories, and repositories from various sources like GitHub and Gitiles. It also provides a web interface, a JSON API, and a command-line interface for searching the indexed repositories.\n\nThe provided context does not mention anything about the license used for this project. To fully answer why this project uses the MIT license, I would need additional context, such as:\n\n1. The license file or license information in the codebase or project documentation.\n2. Any comments or discussions from the project maintainers or contributors regarding the choice of license.\n3. Information about the project's origin, sponsorship, or affiliation that might influence the license choice.\n\nWithout this additional context, I can only speculate about the reasons for choosing the MIT license, which could include factors like promoting widespread adoption, compatibility with other projects or dependencies, or aligning with the project's goals and contributors' preferences.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:25 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:23.259Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 46a160a4ef52f1d2124d363558d7763e
      _order: 0
      cache: {}
      request:
        bodySize: 32185
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-098b6efd7426c768fc7cf90388ebee01-3f4c45a778f7a0c4-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: See zoekt repo
                  find location of tensor function"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 563217
        content:
          mimeType: text/event-stream
          size: 563217
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, Zoekt is a fast text search engine intended for use with source code. It supports indexing and searching code repositories, with features like regular expression matching, file type filtering, and branch/commit filtering.\n\nThe key components and functionalities exposed in the provided code are:\n\n1. **Indexing**: Zoekt can index directories, Git repositories, and repositories hosted on services like Gitiles.\n\n2. **Searching**: The `Search` function in `eval.go` performs the core search operation. It takes a query (`query.Q`), search options, and returns search results (`SearchResult`).\n\n3. **Query Simplification**: The `simplify` function in `eval.go` simplifies and optimizes the query for efficient evaluation.\n\n4. **Match Tree Construction**: Zoekt constructs a \"match tree\" data structure from the query, which is used for efficient matching against the indexed content.\n\n5. **Result Scoring and Ranking**: Zoekt supports scoring and ranking of search results based on factors like repository priority, match positions, and experimental BM25 scoring.\n\n6. **Result Truncation and Limiting**: The search results can be truncated based on configurable limits, such as maximum document count, maximum match count, and context lines.\n\n7. **Repository Listing**: The `List` function in `eval.go` provides a way to list repositories matching a given query.\n\n8. **Regular Expression Handling**: Zoekt has utilities for converting regular expressions to match trees, enabling efficient regular expression matching.\n\nTo fully answer your question about finding the location of the \"tensor\" function, I would need additional context, such as:\n\n1. Is \"tensor\" a regular expression pattern or a literal string?\n2. Are you looking for a specific function named \"tensor\", or any occurrences of the word \"tensor\"?\n3. Do you want to search within a specific repository, or across all indexed repositories?\n4. Are there any additional constraints or filters you want to apply, such as file types or programming languages?\n\nWith this additional context, I can better understand your search requirements and provide more specific guidance on how to use Zoekt's search capabilities to find the desired occurrences of \"tensor\" in the codebase.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:34 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:30.145Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 0f5aa569e89becbfd211fbcf147790c2
      _order: 0
      cache: {}
      request:
        bodySize: 17541
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-5a380f0cd2ba400911531933ae176cdb-01aef5ab79d4e63b-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Explain the
                  logic in src/agent.go, particularly how agents interact with
                  ranking"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 63050
        content:
          mimeType: text/event-stream
          size: 63050
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context from the `limit.go` file, I cannot provide an overview or explanation related to the logic in `src/agent.go` or how agents interact with ranking. The given code context is focused on implementing functionality to limit the number of matches and files displayed in search results based on the provided search options, such as `MaxDocDisplayCount` and `MaxMatchDisplayCount`.\n\nWithout additional context from files like `src/agent.go`, I cannot provide a meaningful explanation of the agent logic or how it interacts with ranking. If you can provide the relevant code context from `src/agent.go` and any other related files, I would be able to analyze and explain that logic.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:42 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:40.199Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 1311d4075457f08418869cc3135239b3
      _order: 0
      cache: {}
      request:
        bodySize: 5075
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-73c3d17385641ee7dea28e50b093a7d6-3cf9997e192814aa-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: What does this
                  repo do??"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 396833
        content:
          mimeType: text/event-stream
          size: 396833
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, this code appears to be part of a larger project called \"zoekt\", which is likely a search or indexing tool. The key functionality shown in this file is:\n\n1. `SortAndTruncateFiles`: This function takes a list of `FileMatch` objects, sorts them, and then truncates the list based on the configured search options.\n\n2. `DisplayTruncator`: This is a stateful function that enforces limits on the number of documents and matches to be displayed. It returns a function that can be used to truncate the list of `FileMatch` objects, ensuring that the display limits are not exceeded.\n\n3. `limitMatches`: This function is used by `DisplayTruncator` to limit the number of matches (either chunk matches or line matches) for each `FileMatch` object, in order to respect the configured display limits.\n\nThe code seems focused on managing the display of search results, ensuring that the number of documents and matches shown to the user does not exceed the configured limits. This suggests that the larger \"zoekt\" project is likely a search or indexing tool that needs to efficiently handle and display large amounts of search results.\n\nTo fully understand the purpose and functionality of this codebase, it would be helpful to have additional context, such as:\n\n1. What is the overall purpose and use case of the \"zoekt\" project?\n2. How does this `limit.go` file fit into the larger codebase and architecture?\n3. What are the specific use cases or requirements that necessitate the display limit functionality implemented in this file?\n4. Are there any other key components or features of the \"zoekt\" project that would help provide a more complete understanding of its purpose and capabilities?\n\nWith this additional context, I could provide a more comprehensive overview of what this codebase does and how it fits into the larger \"zoekt\" project.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:46 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:45.703Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 601192e23d061e7c7a5d057857ef3557
      _order: 0
      cache: {}
      request:
        bodySize: 10363
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-195b37296298ddc2f02b79173bb0e752-5fdd47eb0676b50a-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: describe my
                  code"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 275353
        content:
          mimeType: text/event-stream
          size: 275353
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, this code appears to be part of the Zoekt search engine, which is a fast text search engine intended for use with source code. The code is written in Go and is part of the Sourcegraph fork of the original Zoekt project.\n\nThe main functionality of the code is to handle the display and truncation of search results, ensuring that the number of displayed documents and matches are limited based on the search options specified. \n\nThe key components are:\n\n1. `SortAndTruncateFiles`: This function takes a list of `FileMatch` objects, sorts them, and then truncates the list based on the search options to ensure that the display limits are not exceeded.\n\n2. `DisplayTruncator`: This is a stateful function that enforces the document and match display limits by truncating and mutating the list of `FileMatch` objects. It returns a function that can be used to perform this truncation.\n\n3. `limitMatches`: This function is responsible for limiting the number of matches (either chunk matches or line matches) within each `FileMatch` object, ensuring that the display limits are not exceeded.\n\n4. `limitChunkMatches` and `limitLineMatches`: These are helper functions that handle the actual truncation of the chunk matches and line matches, respectively, within a `FileMatch` object.\n\nOverall, this code seems to be a crucial part of the Zoekt search engine, responsible for ensuring that the search results are displayed in a concise and user-friendly manner, according to the specified search options.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:51 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:50.020Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 0cb35a079ed802d899b9210eaff7ccd1
      _order: 0
      cache: {}
      request:
        bodySize: 24005
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-027433ee74f61841ec26e335be49ec4d-284cdee9c5539346-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: @zoekt
                  describe my code"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 389013
        content:
          mimeType: text/event-stream
          size: 389013
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, this appears to be a part of a search engine implementation called Zoekt. The code is primarily focused on handling sharded search functionality, including the following key components:\n\n1. `shards_test.go`:\n   - Contains test cases for sharded searches, including `testShardedSearch` and `testShardedStreamSearch` functions.\n   - Provides utility functions like `mkSearchResult` and `createMockSearchResult` to create mock search result data.\n   - Includes a `BenchmarkShardedSearch` function to benchmark the performance of sharded searches.\n   - Defines `rankSearcher` and `crashSearcher` structs for testing purposes.\n\n2. `eval.go`:\n   - Implements the `typeRepoSearcher` struct, which handles search queries that involve type and repository filters.\n   - Provides `StreamSearch` and `Search` methods for the `typeRepoSearcher` to perform the corresponding search operations.\n\n3. `eval_test.go`:\n   - Contains tests for the `typeRepoSearcher` functionality, including various search queries with type and repository filters.\n\n4. `eval.go`:\n   - Defines the `indexData` struct, which appears to be the main implementation of the search engine.\n   - Provides the `Search` method for the `indexData` struct, which handles the core search logic, including query simplification, match tree construction, and file matching.\n   - Includes helper functions like `branchIndex` to retrieve information from the index data.\n\nOverall, the provided codebase context suggests that this is a part of a larger search engine implementation that supports sharded search, type and repository filtering, and ranking of search results. To fully understand and describe the code, additional context, such as the purpose of the overall system, the relationship between the different components, and the broader architectural design, would be helpful.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:05:55 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:54.189Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 047a100b419f8f54c256c74ef9a925bf
      _order: 0
      cache: {}
      request:
        bodySize: 3035
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-ee9af02c8ec780771e85aefdfc0e3ed8-39b6b331d76ed36b-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/aggregate.go:

                  ```go

                  var metricFinalAggregateSize = promauto.NewHistogramVec(prometheus.HistogramOpts{
                  	Name:    "zoekt_final_aggregate_size",
                  	Help:    "The number of file matches we aggregated before flushing",
                  	Buckets: prometheus.ExponentialBuckets(1, 2, 20),
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: is my code
                  base clean?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 193686
        content:
          mimeType: text/event-stream
          size: 193686
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, here is a high-level overview:\n\nThe codebase appears to be related to a text search engine called \"Zoekt\", which is a fork of the original Google project and is now the main maintained source. The codebase includes:\n\n1. `aggregate.go` file:\n   - This file appears to contain a Prometheus histogram metric called `zoekt_final_aggregate_size`, which is used to track the number of file matches that are aggregated before flushing.\n\n2. README.md file:\n   - This file provides instructions for downloading, indexing, and searching using the Zoekt search engine.\n   - It covers how to install the necessary commands (`zoekt-index`, `zoekt-git-index`, `zoekt-repo-index`, `zoekt-mirror-gitiles`, `zoekt-webserver`) and how to use the web interface and JSON API for searching.\n\nBased on this limited context, it's difficult to assess the overall cleanliness of the codebase. Additional information, such as the size of the codebase, the number of files, the coding practices used, the presence of tests, and the overall project structure, would be needed to make a more comprehensive evaluation. Please provide any additional context you can share, and I'll be happy to analyze the codebase further.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:00 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:05:59.223Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 380612f963b37149e12ca3bb195c308f
      _order: 0
      cache: {}
      request:
        bodySize: 10253
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-a9fb0546aeb8f6a070ec08ef7804494c-94082a74d30dc3a2-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Can you look
                  through the files and identify the conflicting packages that
                  may be causing this?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 183418
        content:
          mimeType: text/event-stream
          size: 183418
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, here's a high-level overview:\n\nThe code is part of the `zoekt` package, which appears to be a fast text search engine intended for use with source code. The main components are:\n\n1. `SortAndTruncateFiles`: A function that sorts the search results and then truncates them based on the search options.\n2. `DisplayTruncator`: A stateful function that enforces display limits on the search results by truncating and mutating the data.\n3. `limitChunkMatches` and `limitLineMatches`: Helper functions that limit the number of chunk and line matches in the search results.\n\nThe code is focused on managing the display of search results, ensuring that the number of documents and matches shown adheres to the specified limits in the search options.\n\nWithout any additional context about the overall codebase structure or any potential issues, I cannot identify any conflicting packages that may be causing problems. The provided code seems to be self-contained within the `zoekt` package and does not reference any external packages or dependencies.\n\nTo fully answer your question, I would need more information about the broader codebase, any error messages or unexpected behavior, and any other relevant context that could help me identify potential conflicts or issues.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:03 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:02.409Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: bdaafec510d497ec04d239972fd0f095
      _order: 0
      cache: {}
      request:
        bodySize: 10395
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-4e8c4436fd59a9d019efd3547e0a1f35-b01c77dc8ee4b18b-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Mind taking a
                  second look at the file? @limit.go"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 340258
        content:
          mimeType: text/event-stream
          size: 340258
          text: >+
            event: completion

            data: {"completion":"Sure, let me take a closer look at the `limit.go` file.\n\nFrom the code provided, it seems this file contains utility functions related to limiting the display of search results in a text search engine, specifically the `Zoekt` project.\n\nThe main functions in this file are:\n\n1. `SortAndTruncateFiles`: This function takes a slice of `FileMatch` (search results) and a `SearchOptions` struct, sorts the results, and then truncates them based on the limits specified in the `SearchOptions`.\n\n2. `NewDisplayTruncator`: This function returns a `DisplayTruncator` function that can be used to enforce the document and match display limits specified in the `SearchOptions`. The `DisplayTruncator` function takes a slice of `FileMatch` and returns a truncated slice based on the limits, along with a boolean indicating if there are more results to display.\n\n3. `limitMatches`: This function takes a slice of `FileMatch` and a limit, and applies the appropriate limit function (`limitChunkMatches` or `limitLineMatches`) to each `FileMatch` object, returning the truncated slice and the remaining limit.\n\n4. `limitChunkMatches`: This function truncates the `ChunkMatches` field of a `FileMatch` object based on the specified limit, updating the `Content`, `Ranges`, and `SymbolInfo` fields accordingly.\n\n5. `limitLineMatches`: This function truncates the `LineMatches` field of a `FileMatch` object based on the specified limit.\n\nThe overall purpose of this code seems to be to provide a way to limit the display of search results in a space-efficient manner, ensuring that the most relevant information is shown to the user while respecting the specified limits.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:07 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:06.293Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: f9613c540a748d6b3abdc3ed74a99ae4
      _order: 0
      cache: {}
      request:
        bodySize: 10201
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-5fbcab7f40954e4bf1d8abdd2b85b135-e756b7ae6af6fcbb-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Why does this
                  project use the MIT license?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 188985
        content:
          mimeType: text/event-stream
          size: 188985
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, this project seems to be a fast text search engine, specifically intended for searching source code. The project is a fork of the Google-maintained \"zoekt\" project, and is now the main maintained source of Zoekt.\n\nThe code provided focuses on the functionality of limiting and truncating search results, specifically the `SortAndTruncateFiles`, `NewDisplayTruncator`, `limitChunkMatches`, and `limitLineMatches` functions. These functions seem to be responsible for enforcing limits on the number of documents and matches displayed in the search results, in order to provide a manageable and efficient user experience.\n\nHowever, the codebase context does not provide any information about the licensing of this project. The README.md file does not mention the license, and the source code files also do not contain any licensing information. Without additional context about the project's history, development, and the original licensing of the \"zoekt\" project, I cannot provide a definitive answer as to why this project uses the MIT license.\n\nTo fully answer this question, I would need more information about the project's origins, the rationale behind the decision to use the MIT license, and any relevant discussions or documentation that may exist regarding the licensing choice.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:11 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:10.123Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d5b6687c764fe278e0193a12a2d27037
      _order: 0
      cache: {}
      request:
        bodySize: 10206
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-9a6bb6ecf4255488f837652c2c83f241-7c2b139bd3a626be-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: See zoekt repo
                  find location of tensor function"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 236461
        content:
          mimeType: text/event-stream
          size: 236461
          text: >+
            event: completion

            data: {"completion":"Okay, based on the provided codebase context, here's a high-level overview:\n\nThe code appears to be part of the \"zoekt\" project, which is a fast text search engine intended for use with source code. The `limit.go` file contains functions related to limiting the number of results returned by a search.\n\nThe `SortAndTruncateFiles` function is a convenience function that sorts and then truncates the search results based on the specified search options.\n\nThe `NewDisplayTruncator` function returns a stateful function that enforces the document and match display limits specified in the search options. This function is responsible for truncating the search results to fit within the specified limits.\n\nThe `limitChunkMatches` and `limitLineMatches` functions are used by the `NewDisplayTruncator` function to limit the number of chunk matches and line matches, respectively, within each file match.\n\nTo fully answer the question \"See zoekt repo find location of tensor function\", I would need additional context, such as:\n\n1. Is the \"tensor function\" a specific function that exists in the zoekt codebase?\n2. If so, where is this function located (e.g., in which file, class, or package)?\n3. What is the purpose of this \"tensor function\" and how does it relate to the overall functionality of the zoekt project?\n\nWith this additional context, I can provide a more detailed and helpful answer.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:14 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:13.266Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d1c671da394377649120031c69d78a48
      _order: 0
      cache: {}
      request:
        bodySize: 17540
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-869623bfa83784e09961256f86b34b6f-7d5c0aa203367370-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Explain the
                  logic in src/agent.go, particularly how agents interact with
                  ranking"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 320056
        content:
          mimeType: text/event-stream
          size: 320056
          text: >+
            event: completion

            data: {"completion":"Unfortunately, I don't have access to the `src/agent.go` file, as the provided codebase context is limited to the `shards/shards_test.go` and `limit.go` files. \n\nBased on the provided context, I can give you a high-level overview of the code:\n\n1. `shards/shards_test.go`:\n   - This file contains tests for the `shardedSearcher` component, which is responsible for managing and searching across multiple shards (indices) of data.\n   - The tests cover the ranking and document ranking functionality of the `shardedSearcher`.\n   - The `TestShardedSearcher_Ranking` and `TestShardedSearcher_DocumentRanking` tests demonstrate how the `shardedSearcher` ranks and orders the search results based on repository priority and document ranking.\n\n2. `limit.go`:\n   - This file contains functions related to limiting and truncating the number of search results displayed.\n   - The `SortAndTruncateFiles` function first sorts the search results and then truncates them based on the provided `SearchOptions`.\n   - The `NewDisplayTruncator` function creates a stateful function that enforces the document and match display limits specified in the `SearchOptions`.\n   - The `limitChunkMatches` and `limitLineMatches` functions are used to limit the number of chunk matches and line matches, respectively, in the search results.\n\nWithout the `src/agent.go` file, I don't have enough context to explain the logic in that file, particularly how agents interact with ranking. If you can provide the `src/agent.go` file or any additional relevant context, I'd be happy to take another look and provide a more detailed explanation.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:18 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:16.982Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 100867c7c3c54848aac130af532b2c1f
      _order: 0
      cache: {}
      request:
        bodySize: 890
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-83d908243f5faa3e42b9c3fc3fd36684-51e25a8d27b99301-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>@zoekt describe my code</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 41362
        content:
          mimeType: text/event-stream
          size: 41362
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ezoekt\u003c/value\u003e\n\u003cvariants\u003esearch search-engine indexer\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode\u003c/value\u003e\n\u003cvariants\u003ecodebase repository source-code\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003edescribe\u003c/value\u003e\n\u003cvariants\u003eanalyze explain summarize\u003c/variants\u003e\n\u003cweight\u003e0.7\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 18:10:39 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T18:10:38.782Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 55f7489225d72144bd4409117cfc3f73
      _order: 0
      cache: {}
      request:
        bodySize: 889
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-77029fc1963d54b26f7c551c49652d60-cd785a5bacdabf71-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>is my code base clean?</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 78858
        content:
          mimeType: text/event-stream
          size: 78858
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode quality\u003c/value\u003e\n\u003cvariants\u003ecode cleanliness code hygiene code health\u003c/variants\u003e\n\u003cweight\u003e0.9\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode analysis\u003c/value\u003e\n\u003cvariants\u003ecode review code inspection code audit\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode metrics\u003c/value\u003e\n\u003cvariants\u003ecode complexity code duplication code coverage\u003c/variants\u003e\n\u003cweight\u003e0.7\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003erefactoring\u003c/value\u003e\n\u003cvariants\u003ecode refactoring technical debt\u003c/variants\u003e\n\u003cweight\u003e0.6\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 18:20:52 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T18:20:51.383Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 3ffa6170ce25982a645850caa404e0fd
      _order: 0
      cache: {}
      request:
        bodySize: 947
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-64e2298fe8b685ff10eb93392a2731a4-6a2f4ba4537f2276-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>Explain the logic in src/agent.go,
                  particularly how agents interact with ranking</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 39612
        content:
          mimeType: text/event-stream
          size: 39612
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eagent\u003c/value\u003e\n\u003cvariants\u003eagents\u003c/variants\u003e\n\u003cweight\u003e0.9\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eranking\u003c/value\u003e\n\u003cvariants\u003erank rank_agent rank_agents\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003esrc/agent.go\u003c/value\u003e\n\u003cvariants\u003eagent.go\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 18:21:01 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T18:21:00.769Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: e85ef2794f837de58f8c55912aac22db
      _order: 0
      cache: {}
      request:
        bodySize: 5170
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-bd84ac7efb7bfef3721776692abee547-ec05e8d8decda0fe-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: What does this
                  repo do??"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 22312
        content:
          mimeType: text/event-stream
          size: 22312
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, it appears to be a Go package related to searching and displaying search results. The code includes functions for sorting and truncating file matches based on search options, enforcing limits on document and match display, and limiting the number of chunk matches and line matches in file matches. The package seems to provide functionality for processing and displaying search results in a structured manner.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:22 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:21.356Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: fb52e07508432f00d45143780ee5f6e9
      _order: 0
      cache: {}
      request:
        bodySize: 10458
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-9c7d94f1147d6ed934d2c17921b21c02-66456d9177b336f5-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: describe my
                  code"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 73999
        content:
          mimeType: text/event-stream
          size: 73999
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context from the file limit.go, it appears to be a part of the Sourcegraph codebase related to sorting and truncating file matches based on search options. The code defines functions for sorting and truncating files, creating a display truncator, and limiting the number of match results displayed in the search output. The code handles scenarios where there are limits set for displaying document counts and match counts, and it enforces these limits while truncating the results accordingly. Additionally, there are functions for limiting chunk matches and line matches within file matches.\n\nFor a complete description of the code and its functionality, I would need additional context about the overall structure and purpose of the Sourcegraph codebase, as well as any other relevant code files or dependencies.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:24 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:23.350Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 0cdd89588342351c942976ec8cba8dba
      _order: 0
      cache: {}
      request:
        bodySize: 24100
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-79106c6b6ad822cfe5d119ec95c92035-bbf8f1c2a9bd5a6a-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: @zoekt
                  describe my code"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 66094
        content:
          mimeType: text/event-stream
          size: 66094
          text: >+
            event: completion

            data: {"completion":"Based on the provided code from the file `eval.go`, we can see a function `Search` defined on the `indexData` struct. This function takes a query, search options, and context as input parameters, and it performs searching on the index data based on the query. The function applies default search options if not set, simplifies the query, constructs a match tree, prunes the match tree, and then searches for matches in the index data. It iterates over documents, filters based on various conditions such as repository, and constructs file matches. It also calculates scores, gathers matches, fills in context lines, and stores match counts and other statistics related to the search. Finally, it sorts and truncates the results if document ranking is enabled.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:28 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:27.180Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: cc9f941fc1a6332ab621acce983505db
      _order: 0
      cache: {}
      request:
        bodySize: 3130
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-a88b0899f71100b4c823e92f127c0c5a-f4ee4a2a834e3c6f-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/aggregate.go:

                  ```go

                  var metricFinalAggregateSize = promauto.NewHistogramVec(prometheus.HistogramOpts{
                  	Name:    "zoekt_final_aggregate_size",
                  	Help:    "The number of file matches we aggregated before flushing",
                  	Buckets: prometheus.ExponentialBuckets(1, 2, 20),
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: is my code
                  base clean?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 46955
        content:
          mimeType: text/event-stream
          size: 46955
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, it appears that the codebase is for a project called \"Zoekt,\" which is a fast text search engine designed for use with source code. The code seems to include instructions for downloading, indexing (for directories, Git repositories, and Repo repositories), and searching using tools provided in the Zoekt project. The README.md file provides details on installation steps, usage instructions, and available interfaces for searching, such as a web interface and JSON API.\n\nTo determine if the codebase is clean, I would need to inspect the actual code implementation in addition to the provided context.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:31 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:30.611Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 235fa231f3412000c870b2c4666af24f
      _order: 0
      cache: {}
      request:
        bodySize: 10348
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-a6e722b484dbed2d6973a296f9a85e28-e772981728a926c1-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Can you look
                  through the files and identify the conflicting packages that
                  may be causing this?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 87740
        content:
          mimeType: text/event-stream
          size: 87740
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, the `limit.go` file in the `zoekt` package contains functions related to sorting and truncating file matches based on search options. The `SortAndTruncateFiles` function sorts and truncates file matches according to specified search options. The `DisplayTruncator` type and the `NewDisplayTruncator` function are used to enforce limits on the display of documents and matches. The `limitMatches`, `limitChunkMatches`, and `limitLineMatches` functions are used to limit the number of chunk matches and line matches in file matches. Additionally, there is some logic to handle truncating content based on line numbers.\n\nBased on this overview, the file handles sorting and truncation of file matches as well as enforcing display limits based on search options. It also includes functions for limiting chunk matches and line matches in file matches.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:34 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:33.728Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: e79b5dc90cca11505ef150089e82abe1
      _order: 0
      cache: {}
      request:
        bodySize: 10490
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-03e1ad0e59c62073040310bc5a5d640f-433f37e8c5cbb71b-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Mind taking a
                  second look at the file? @limit.go"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 78851
        content:
          mimeType: text/event-stream
          size: 78851
          text: >+
            event: completion

            data: {"completion":"Based on the given context from the file `limit.go`, it appears to contain functions related to sorting and truncating files based on search options in the context of a search engine. \n\nThe file defines a `SortAndTruncateFiles` function that sorts and truncates files based on search options, a `DisplayTruncator` function for enforcing display limits, and a `NewDisplayTruncator` function for creating a display truncator based on search options.\n\nAdditionally, there are functions `limitMatches`, `limitChunkMatches`, and `limitLineMatches` that handle limiting the number of matches or line fragments in file matches based on specified limits.\n\nThese functions seem to play a role in managing the display of search results by limiting the number of matches or lines displayed, as well as sorting files within a search context.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:38 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:37.480Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d4a621690bc873a91f77029e3088609c
      _order: 0
      cache: {}
      request:
        bodySize: 10296
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-5f3a1e90f68093f788fd5e42863037f0-001bb918e4abbc61-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Why does this
                  project use the MIT license?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 39231
        content:
          mimeType: text/event-stream
          size: 39231
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, the project seems to be a search engine implementation called \"Zoekt.\" The code includes functions for sorting and truncating file matches based on search options, as well as enforcing limits on the display of documents and matches. The code also includes functions for limiting chunk matches and line matches within file matches.\n\nTo fully answer the question of why this project uses the MIT license, additional context such as the project's licensing information, contributors' intentions, or any specific reasons mentioned in project documentation would be required.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:48 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:47.443Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 36fe5ea1643a3faaa59fbe21154e6955
      _order: 0
      cache: {}
      request:
        bodySize: 10301
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-f98cfbcc44edee0c91a8ba59bdfa12d1-3f7b6cb02914a934-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: See zoekt repo
                  find location of tensor function"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 76780
        content:
          mimeType: text/event-stream
          size: 76780
          text: >+
            event: completion

            data: {"completion":"Based on the provided codebase context, the code includes functions related to sorting and truncating file matches in a search engine called Zoekt. The `SortAndTruncateFiles` function sorts and truncates file matches based on search options. The `DisplayTruncator` function enforces display limits for documents and matches by truncating and mutating the input before returning the results. The code also includes functions for limiting the number of chunk matches and line matches in file matches.\n\nTo find the location of the `tensor` function in the Zoekt repository, we would need to search through the codebase for the definition of the `tensor` function. More context about the structure of the Zoekt repository and the specific code organization would be required to pinpoint the exact location of the `tensor` function.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:50 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:50.022Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 4454790ec53f0d5ddaadd1800f074bdb
      _order: 0
      cache: {}
      request:
        bodySize: 17635
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-75412d7f76d45bc9398f6874658da855-8417529d0d8d352f-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided codebase context are the code you need and have access to.
                  Do not make any assumptions. Give me a high-level overview
                  based on the given context first before asking for additional
                  context to answer the following question fully: Explain the
                  logic in src/agent.go, particularly how agents interact with
                  ranking"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 53640
        content:
          mimeType: text/event-stream
          size: 53640
          text: >+
            event: completion

            data: {"completion":"The provided code is related to sorting and truncating search results in a text search engine called Zoekt. The code includes functions for sorting and truncating files based on search options, creating a display truncator based on document and match display limits, and limiting the number of chunk matches and line matches in file matches.\n\nTo explain the logic in src/agent.go, particularly how agents interact with ranking, I would need to know additional context about the agent functionality and how ranking is defined and calculated within the Zoekt search engine. Additional context on how agents utilize ranking information and interact with the search results would be necessary to provide a detailed explanation.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 19:06:54 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T19:06:53.537Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 12581f1c735a04aeb88af7a54cd007b2
      _order: 0
      cache: {}
      request:
        bodySize: 217
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "217"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 340
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CodyConfigFeaturesResponse {
                  site {
                      codyConfigFeatures {
                          chat
                          autoComplete
                          commands
                          attribution
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CodyConfigFeaturesResponse
            value: null
        url: https://sourcegraph.com/.api/graphql?CodyConfigFeaturesResponse
      response:
        bodySize: 155
        content:
          encoding: base64
          mimeType: application/json
          size: 155
          text: "[\"H4sIAAAAAAAAAzyLwQqA\",\"IBAF/2XPfYFXof/YdC0h3dDnIcR/Dws6DQwznTyDyXSq\
            ETLp1N9Wc4j7KoxWpL72YJBBabIQN6jVdJ0yj885TYmzr38DlLg1RM1kAp9VxhjjAQA\
            A//8DAIfOLkJuAAAA\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.152Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 09b59ac55ce3c40d6f9ab8c79846a2c6
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 244
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query ContextFilters {
                  site {
                      codyContextFilters(version: V1) {
                          raw
                      }
                  }
              }
            variables: {}
        queryString:
          - name: ContextFilters
            value: null
        url: https://sourcegraph.com/.api/graphql?ContextFilters
      response:
        bodySize: 107
        content:
          encoding: base64
          mimeType: application/json
          size: 107
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqzixJBdHJ+SmVzvl5JakVJW6ZOSWpRcUg0aLE\
            ciWrvNKcnNra2loAAAAA//8DADYuyGU1AAAA\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.372Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 9d0e130a38eaf9d3cd8d9512afc14b87
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 328
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query ContextFilters {
                  site {
                      codyContextFilters(version: V1) {
                          raw
                      }
                  }
              }
            variables: {}
        queryString:
          - name: ContextFilters
            value: null
        url: https://sourcegraph.com/.api/graphql?ContextFilters
      response:
        bodySize: 104
        content:
          encoding: base64
          mimeType: application/json
          size: 104
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdHJ+SmVzvl5JakVJW6ZOSWpRcUg0aLEciWrv\
            NKcnNra2loAAAAA//8DADYuyGU1AAAA\"]"
          textDecoded:
            data:
              site:
                codyContextFilters:
                  raw: null
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.271Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 460fa85fef77ffa15bb82fa3a88049a3
      _order: 0
      cache: {}
      request:
        bodySize: 318
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "318"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 345
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmConfiguration {
                  site {
                      codyLLMConfiguration {
                          chatModel
                          chatModelMaxTokens
                          fastChatModel
                          fastChatModelMaxTokens
                          completionModel
                          completionModelMaxTokens
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmConfiguration
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmConfiguration
      response:
        bodySize: 248
        content:
          encoding: base64
          mimeType: application/json
          size: 248
          text: "[\"H4sIAAAAAAAAA3zOwQqCQBAG4HeZs+K0BqFXr3rrBYbdMRdtJ3ZXKmTfPSyiSOg08PP/H\
            7OAoUhQLxBs5PVqMfe27RpxvT3NnqIV98wHip0YnqAGcnHwcrG60BPNhvMyD+Icx1yh\
            2qNSFWSfQUe3o4zsAtQ7hYgZ9BRi898byI7ziyvxAD+bLanlfJl4ffaN9tbzVfwYihD\
            JazHsYdP7gipETCmlBwAAAP//AwBQeP+1EwEAAA==\"]"
          textDecoded:
            data:
              site:
                codyLLMConfiguration:
                  chatModel: anthropic/claude-3-sonnet-20240229
                  chatModelMaxTokens: 12000
                  completionModel: fireworks/starcoder
                  completionModelMaxTokens: 9000
                  fastChatModel: anthropic/claude-3-haiku-20240307
                  fastChatModelMaxTokens: 12000
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.492Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 7edf90ea650cb304fe26f9d57bd79477
      _order: 0
      cache: {}
      request:
        bodySize: 165
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "165"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 345
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmConfiguration {
                  site {
                      codyLLMConfiguration {
                          smartContextWindow
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmConfiguration
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmConfiguration
      response:
        bodySize: 142
        content:
          encoding: base64
          mimeType: application/json
          size: 142
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqzixJBdHJ+SmVPj6+zvl5aZnppUWJJZn5eWD5\
            3MSiEuf8vJLUipLwzLyU/HIlK6XUvMSknNQUpdra2loAAAAA//8=\",\"AwDoCDSlSw\
            AAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.513Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d890a82b3cc2fe4d514f289e8fe6d158
      _order: 0
      cache: {}
      request:
        bodySize: 150
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "150"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 340
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmProvider {
                  site {
                      codyLLMConfiguration {
                          provider
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmProvider
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmProvider
      response:
        bodySize: 128
        content:
          encoding: base64
          mimeType: application/json
          size: 128
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdHJ+SmVPj6+zvl5aZnppUWJJZn5eSDxgqL8s\
            syU1CIlK6Xi/NKi5NT0osSCDKXa2tpaAAAAAP//AwAfFAXARQAAAA==\"]"
          textDecoded:
            data:
              site:
                codyLLMConfiguration:
                  provider: sourcegraph
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.493Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 7f3ff10a39505669cb5d36145df5fa95
      _order: 0
      cache: {}
      request:
        bodySize: 227
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "227"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 325
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentUser {
                  currentUser {
                      id
                      hasVerifiedEmail
                      displayName
                      username
                      avatarURL
                      primaryEmail {
                          email
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentUser
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentUser
      response:
        bodySize: 355
        content:
          encoding: base64
          mimeType: application/json
          size: 355
          text: "[\"H4sIAAAAAAAAA2RPy04=\",\"g0AU/Ze7hoJptDBJE21tXVSJj5TU5WW4hQGGwXlUKeHf\
            G1ITF+7OyTk5jwFytAhsAO60ptbuDemJihwYpIek4ZU6J49vNy8VX4IHJZqUtDgKyjc\
            SRQPMakce5MJ0DfYJSgIGH8ppToXGrlwp68dhGIIHzpBurwbzZ8iUjWv/2H5LBx7gCS\
            3q/fszMCit7QwLgqaczwqlioamBK5aS62dcSUDDB7WRaT4botf2Se5VZ1Vt/l2c/6Js\
            kMa4ULMTZrt1slrungKXX+qlya+8zl40GkhUfe/JwagK/i37L6YhKkNxnEcLwAAAP//\
            AwC04S4cMQEAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.534Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 84b962509b12000d0eef7c8a8fa655f3
      _order: 0
      cache: {}
      request:
        bodySize: 268
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "268"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 341
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentUserCodySubscription {
                  currentUser {
                      codySubscription {
                          status
                          plan
                          applyProRateLimits
                          currentPeriodStartAt
                          currentPeriodEndAt
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentUserCodySubscription
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentUserCodySubscription
      response:
        bodySize: 228
        content:
          encoding: base64
          mimeType: application/json
          size: 228
          text: "[\"H4sIAAAAAAAAA1zMsQrCMBSF4Xc5c4U2VodsRToIgqWtDm6xyRCoSbi5GUrJu4uCoI7n5\
            +Os0IoV5IopERnHl2joPb1ehnSPE9nA1rtXi6w4RUg0h/F4bVEgzMpBouvPKKBCmJeO\
            fK/YnOzDcoRkSqb4fHeGrNcDK+KGISFKUW/K3aaqRyFkVcmtuOFPt05/2f2vzTnnJwA\
            AAP//AwBSFe7+wgAAAA==\"]"
          textDecoded:
            data:
              currentUser:
                codySubscription:
                  applyProRateLimits: true
                  currentPeriodEndAt: 2024-06-14T22:11:32Z
                  currentPeriodStartAt: 2024-05-14T22:11:32Z
                  plan: PRO
                  status: ACTIVE
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.818Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: ea3ddfc16f0b52a2180a9bd21c87dc08
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 240
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query Repository($name: String!) {
              	repository(name: $name) {
              		id
              	}
              }
            variables:
              name: github.com/sourcegraph/cody
        queryString:
          - name: Repository
            value: null
        url: https://sourcegraph.com/.api/graphql?Repository
      response:
        bodySize: 123
        content:
          encoding: base64
          mimeType: application/json
          size: 123
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqSi3IL84syS+qBPEyU5SslEJzw8qTjP0KUtwt\
            K1ND8o18Q3wr/UJ8K/0dbW2VamtrAQAAAP//AwDHAhygPQAAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.035Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 8d297306aeea324b87ef494954016fba
      _order: 0
      cache: {}
      request:
        bodySize: 164
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "164"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 248
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteIdentification {
              	site {
              		siteID
              		productSubscription {
              			license {
              				hashedKey
              			}
              		}
              	}
              }
            variables: {}
        queryString:
          - name: SiteIdentification
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteIdentification
      response:
        bodySize: 212
        content:
          encoding: base64
          mimeType: application/json
          size: 212
          text: "[\"H4sIAAAAAAAAAzTLsQ6CMBCA4Xe52YX2rgVmF+PI4HztXaWJAdKWwRDf3WDiv/zTd4BwY\
            xgPqLnp/7crjDCte4n6LLzNDw1wga2sssc27aHGkreW1+UErxx1qT87c51V7vqGEYbo\
            u9AZm/okmgxi70QZlZzzqNEaJOMNReocCdkgiCk4j4btwJwIPmdfAAAA//8DABj2u36\
            gAAAA\"]"
          textDecoded:
            data:
              site:
                productSubscription:
                  license:
                    hashedKey: 9c71b123f8fdef24486dea4e56674ec32452725c5165d53bd44fb6742a39aaf5
                siteID: SourcegraphWeb
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:55.993Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: b584bf96a3d88ab96114e5cbea1e4bca
      _order: 0
      cache: {}
      request:
        bodySize: 164
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "164"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 332
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteIdentification {
              	site {
              		siteID
              		productSubscription {
              			license {
              				hashedKey
              			}
              		}
              	}
              }
            variables: {}
        queryString:
          - name: SiteIdentification
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteIdentification
      response:
        bodySize: 212
        content:
          encoding: base64
          mimeType: application/json
          size: 212
          text: "[\"H4sIAAAAAAAAAzTLsQ6CMBCA4Xe52YX2rgVmF+PI4HztXaWJAdKWwRDf3WDiv/zTd4BwY\
            xgPqLnp/7crjDCte4n6LLzNDw1wga2sssc27aHGkreW1+UErxx1qT87c51V7vqGEYbo\
            u9AZm/okmgxi70QZlZzzqNEaJOMNReocCdkgiCk4j4btwJwIPmdfAAAA//8DABj2u36\
            gAAAA\"]"
          textDecoded:
            data:
              site:
                productSubscription:
                  license:
                    hashedKey: 9c71b123f8fdef24486dea4e56674ec32452725c5165d53bd44fb6742a39aaf5
                siteID: SourcegraphWeb
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.151Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 203c1896021c3a09dfe619120ea1b725
      _order: 0
      cache: {}
      request:
        bodySize: 101
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "101"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 248
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteProductVersion {
                  site {
                      productVersion
                  }
              }
            variables: {}
        queryString:
          - name: SiteProductVersion
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteProductVersion
      response:
        bodySize: 136
        content:
          encoding: base64
          mimeType: application/json
          size: 136
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdEFRfkppcklYalFxZn5eUpWSkbm5obmJvFGB\
            kYmugZmugam8aZ6JrpJpuYmxgYG5oYmpqlKtbW1AAAAAP//AwAHA5EmSQAAAA==\"]"
          textDecoded:
            data:
              site:
                productVersion: 277174_2024-06-05_5.4-b5743007145e
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.260Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 2aa42833ae189b030c5bc322f1d27b0c
      _order: 0
      cache: {}
      request:
        bodySize: 101
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "101"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 332
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteProductVersion {
                  site {
                      productVersion
                  }
              }
            variables: {}
        queryString:
          - name: SiteProductVersion
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteProductVersion
      response:
        bodySize: 136
        content:
          encoding: base64
          mimeType: application/json
          size: 136
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdEFRfkppcklYalFxZn5eUpWSkbm5obmJvFGB\
            kYmugZmugam8aZ6JrpJpuYmxgYG5oYmpqlKtbW1AAAAAP//AwAHA5EmSQAAAA==\"]"
          textDecoded:
            data:
              site:
                productVersion: 277174_2024-06-05_5.4-b5743007145e
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.490Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
  pages: []
  version: "1.2"
