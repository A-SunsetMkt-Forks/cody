log:
  _recordingName: chat-response-quality
  creator:
    comment: persister:fs
    name: Polly.JS
    version: 6.0.6
  entries:
    - _id: f1cdaf0fe64120ad5c0ffd4692548236
      _order: 0
      cache: {}
      request:
        bodySize: 5061
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-37cb1fd15eb38f9f3538feb80be096c0-e479f69236bbe905-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: What does this repo do??"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 413942
        content:
          mimeType: text/event-stream
          size: 413942
          text: >+
            event: completion

            data: {"completion":"The provided code is part of a Go package called `zoekt`. The package seems to be responsible for handling search results and enforcing display limits on the search results.\n\nThe main functionality appears to be:\n\n1. `SortAndTruncateFiles` function: This function takes a slice of `FileMatch` structs and a `SearchOptions` struct. It first sorts the `FileMatch` slice using the `SortFiles` function (not shown). Then, it creates a `DisplayTruncator` based on the `SearchOptions`, and uses it to truncate the `FileMatch` slice according to the specified limits. The truncated slice is returned.\n\n2. `DisplayTruncator` type: This is a function type that takes a slice of `FileMatch` and returns a (potentially) truncated slice of `FileMatch` along with a boolean indicating whether there are more matches that weren't included due to the limits.\n\n3. `NewDisplayTruncator` function: This function creates a `DisplayTruncator` based on the provided `SearchOptions`. It checks for limits on the maximum number of files (`MaxDocDisplayCount`) and matches (`MaxMatchDisplayCount`) to display, and creates a `DisplayTruncator` that enforces those limits.\n\n4. `limitMatches` function: This function is a helper function that truncates the matches in each `FileMatch` based on the specified `limit` and whether matches should be chunked (`chunkMatches`).\n\n5. `limitChunkMatches` and `limitLineMatches` functions: These functions implement the logic for truncating matches based on whether they are chunk matches or line matches, respectively.\n\nThe code deals with organizing and limiting search results based on configurable options, but without additional context, it's difficult to determine the specific purpose or use case of this repository.\n\nTo better understand what this repository does, it would be helpful to have more information about the overall project or application this code is a part of.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:08:47 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:08:45.703Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d99d1d6ba63591a12b2d57dde7eebffa
      _order: 0
      cache: {}
      request:
        bodySize: 32327
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-0af2d049ceeef1f371f3456bd2e812db-580aa50aeb9482b9-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: describe my code"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 590692
        content:
          mimeType: text/event-stream
          size: 590692
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, this code appears to be part of a text search engine called Zoekt. Here's a high-level overview of the main components and functionalities:\n\n1. **Search**: The `Search` function in `eval.go` is responsible for performing the actual search operation. It takes a query (`q`) and search options (`opts`) as input and returns a `SearchResult` containing the matched files, repositories, and various statistics.\n\n2. **Query Simplification**: The `simplify` function in `eval.go` simplifies the query by applying optimizations and transformations based on the index data and repository metadata.\n\n3. **Match Tree**: The code constructs a `matchTree` data structure to efficiently evaluate the query against the indexed data. The `regexpToMatchTreeRecursive` function in `eval.go` converts a regular expression to a `matchTree`.\n\n4. **Gather Matches**: The `gatherMatches` function in `eval.go` collects the matching candidates from the document, ensuring that they are sorted, non-overlapping, and potentially merged based on the search options.\n\n5. **Score Calculation**: The `scoreFile` and `scoreFileUsingBM25` functions (not shown in the provided context) are responsible for calculating the score for each matched file.\n\n6. **Repository Management**: The code manages repositories, their metadata (branches, subrepositories, etc.), and provides functionality to list repositories based on a query (`List` function in `eval.go`).\n\n7. **Indexing**: While the indexing process is not directly shown in the provided context, there are hints about indexing repositories (`zoekt-index`, `zoekt-git-index`, `zoekt-repo-index` commands mentioned in `README.md`).\n\n8. **Limiting and Truncation**: The `limit.go` file contains functions for limiting and truncating the search results based on various options (e.g., `MaxDocDisplayCount`, `MaxMatchDisplayCount`).\n\nTo provide a more detailed description of the code, I might need additional context, such as the definitions of the data structures (e.g., `matchTree`, `Repository`, `FileMatch`) and the implementation of some functions (e.g., `newMatchTree`, `scoreFile`, `scoreFileUsingBM25`).","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:08:59 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:08:55.719Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: e1d6729cce069b0a797a3d8fd32c02da
      _order: 0
      cache: {}
      request:
        bodySize: 23991
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-5bb4e867264865df29ae54a839b5f66c-ce3de49dd91c77b4-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: @zoekt describe my code"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 133498
        content:
          mimeType: text/event-stream
          size: 133498
          text: >+
            event: completion

            data: {"completion":"From the provided code context, it appears that this is part of the Zoekt code repository. Zoekt is an open-source code search engine that powers code search on Sourcegraph and other platforms. The provided code snippets are related to the sharding and evaluation functionality of Zoekt.\n\nThe `shards` package contains code for sharding the search index across multiple shards and performing distributed searches across these shards. There are several test cases and benchmarks included in the package for testing the sharded search functionality.\n\nThe `eval` package contains the core logic for evaluating search queries and retrieving search results from the index. The `indexData` struct represents the in-memory search index, and it provides methods for simplifying queries, constructing match trees, gathering matches, scoring results, and performing the actual search.\n\nWithout more context about your specific question, it's difficult to provide a more detailed overview. If you could provide the specific question or area you need clarification on, I can better understand the context and provide a more focused answer.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:10 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:07.672Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 4142534d79c7815815533c2ed4bbd95a
      _order: 0
      cache: {}
      request:
        bodySize: 7797
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-79e9b6ec1d46685c3f8f32b6f1df2ba6-2347a18de4e11a2e-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestSendByRepository(t *testing.T) {
                  	wantStats := zoekt.Stats{}
                  	wantStats.ShardsScanned = 1

                  	// n1, n2, n3 are the number of file matches for each of the 3 repositories in this
                  	// test.
                  	f := func(n1, n2, n3 uint8) bool {
                  		sr := createMockSearchResult(n1, n2, n3, wantStats)

                  		mock := &mockSender{}
                  		sendByRepository(sr, &zoekt.SearchOptions{}, mock)

                  		if diff := cmp.Diff(wantStats, mock.stats); diff != "" {
                  			t.Logf("-want,+got/n%s", diff)
                  			return false
                  		}

                  		nonZero := 0
                  		for _, l := range []uint8{n1, n2, n3} {
                  			if l > 0 {
                  				nonZero++
                  			}
                  		}
                  		if l := len(mock.files); l != nonZero {
                  			t.Logf("wanted results from %d repositores, got %d", nonZero, l)
                  			return false
                  		}

                  		gotTotal := 0
                  		for _, fs := range mock.files {
                  			gotTotal += len(fs)
                  		}
                  		wantTotal := int(n1) + int(n2) + int(n3)
                  		if gotTotal != wantTotal {
                  			t.Logf("wanted %d file matches, got %d", wantTotal, gotTotal)
                  			return false
                  		}

                  		for _, fs := range mock.files {
                  			if len(fs) == 0 {
                  				t.Logf("got search result with 0 file matches after split")
                  				return false
                  			}
                  		}
                  		return true
                  	}

                  	if err := quick.Check(f, nil); err != nil {
                  		t.Error(err)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file grpc/chunk/chunk.go:

                  ```go

                  func SendAll[T proto.Message](sendFunc func([]T) error, items ...T) error {
                  	c := New(sendFunc)

                  	err := c.Send(items...)
                  	if err != nil {
                  		return err
                  	}

                  	return c.Flush()
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: is my code base clean?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 483448
        content:
          mimeType: text/event-stream
          size: 483448
          text: >+
            event: completion

            data: {"completion":"From the provided context, it's difficult to determine if the entire codebase is clean or not as the context is quite limited. However, I can provide an overview of what I can deduce from the given code snippets:\n\n1. `shards/shards_test.go`: This file contains a test function `TestSendByRepository` that appears to be testing the `sendByRepository` function. The test generates mock search results with varying numbers of file matches across multiple repositories and verifies the behavior of `sendByRepository` with these mock results.\n\n2. `shards/rankSearcher.go`: This file defines a `rankSearcher` struct with a `Repository` method that returns a `*zoekt.Repository`.\n\n3. `README.md`: The README file provides instructions for installing, indexing, and searching with Zoekt. It also mentions some fork details, including Sourcegraph-specific changes made to the original Zoekt project.\n\n4. `grpc/chunk/chunk.go`: This file contains a generic function `SendAll` that takes a send function and a variadic slice of items of type `T` (implementing `proto.Message` interface). It creates a new `Chunk` instance, sends the items using the provided send function, and flushes the chunk.\n\nWhile the provided context gives some insights into specific aspects of the codebase, such as testing, indexing, and chunking, it's not sufficient to make a comprehensive assessment of the codebase's cleanliness. To provide a more informed evaluation, I would need additional context, such as:\n\n- The overall project structure and organization\n- Coding conventions and style guidelines\n- Test coverage and quality\n- Code complexity and maintainability metrics\n- Documentation and comment quality\n- Adherence to best practices and design patterns\n- Separation of concerns and modular design\n- Error handling and edge case handling\n- Performance and scalability considerations\n\nWithout access to the broader codebase, it's challenging to determine if the code is clean or not. Please provide additional context or clarify the specific areas of concern regarding code cleanliness if you'd like me to provide a more detailed assessment.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:18 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:17.155Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: bc38555e0f94029a87f5241774e1215e
      _order: 0
      cache: {}
      request:
        bodySize: 10239
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-8e79a704236b5d869861346a1f9ac149-d5cf3b1062700cbf-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Can you look through the
                  files and identify the conflicting packages that may be
                  causing this?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 191018
        content:
          mimeType: text/event-stream
          size: 191018
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, it appears that there are no conflicting packages mentioned. The code seems to be focused on implementing functionality related to limiting and truncating search results based on various options and limits.\n\nThe `limit.go` file contains functions that handle sorting, truncating, and limiting the number of file matches and individual matches (line or chunk-based) returned from a search query. It provides functionality to enforce limits on the maximum number of documents (files) and matches displayed, as well as the ability to chunk matches based on context lines or individual lines.\n\nThe `README.md` file provides an overview of the Zoekt project, which is a fast text search engine intended for use with source code. It includes instructions for downloading, indexing, and searching, as well as information about the web interface, JSON API, and command-line interface.\n\nWithout additional context or information about the specific issue or error you're trying to resolve, it's difficult to identify conflicting packages or other potential sources of conflicts within the provided code.\n\nIf you could provide more details about the problem you're encountering or the specific error message you're seeing, it would be helpful to determine if there are any conflicting packages or dependencies that might be causing issues.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:27 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:25.346Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 46cbe145d20d7297ed5f7aea69de8f91
      _order: 0
      cache: {}
      request:
        bodySize: 32359
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-0a5a9a536d3672a0e39ee5099e911620-0d8b3d7652a2352a-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Mind taking a second look at
                  the file? @limit.go"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 134081
        content:
          mimeType: text/event-stream
          size: 134081
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, here's a high-level overview:\n\n1. `eval.go` contains the core logic for executing search queries and retrieving search results. It defines functions like `Search` and `List` that take a query and search options, and return the search results or a list of repositories.\n\n2. `limit.go` contains utility functions for sorting, truncating, and limiting the search results based on the specified options. It defines functions like `SortAndTruncateFiles`, `NewDisplayTruncator`, `limitChunkMatches`, and `limitLineMatches`.\n\n3. `README.md` is the project's main documentation file, providing instructions for downloading, indexing, and searching with Zoekt, a text search engine primarily intended for source code.\n\n4. `vscode/src/external-services.ts` is a TypeScript file, but its content is not provided in the given context.\n\nWith this overview in mind, if you have a specific question about the `limit.go` file, I can provide a more detailed answer by analyzing that file further.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:36 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:32.696Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: fc9ce247c783c5c149ce1cf692307cc1
      _order: 0
      cache: {}
      request:
        bodySize: 10187
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-dc0d1ea3e57ecf136e78b6aec6bd9a9b-e238b1a6454ba260-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Why does this project use
                  the MIT license?"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 112844
        content:
          mimeType: text/event-stream
          size: 112844
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, I don't have enough information to determine why this project uses the MIT license. The code snippets provided are related to implementing display limits and truncation of search results, but they don't contain any information about licensing.\n\nTo provide an overview of why the Zoekt project uses the MIT license, I would need access to additional context, such as:\n\n1. The LICENSE file or any other file that specifies the licensing terms for the project.\n2. Documentation or comments that explain the rationale behind choosing the MIT license.\n3. Information about the project's origins, contributors, and any organizational policies or guidelines that might have influenced the license choice.\n\nWithout this additional context, I can only speculate about the reasons for using the MIT license, which would not be appropriate. If you can provide the relevant licensing information or documentation, I can then analyze it and provide an informed overview of why the MIT license was chosen for this project.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:42 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:40.393Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 20b2a531d1d50a15cadecd1b43f1a914
      _order: 0
      cache: {}
      request:
        bodySize: 32170
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-15a9d52331ee68099ea2d975f55e6cb4-afceae11539e3cd6-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  // Copyright 2016 Google Inc. All rights reserved.

                  //

                  // Licensed under the Apache License, Version 2.0 (the "License");

                  // you may not use this file except in compliance with the License.

                  // You may obtain a copy of the License at

                  //

                  //    http://www.apache.org/licenses/LICENSE-2.0

                  //

                  // Unless required by applicable law or agreed to in writing, software

                  // distributed under the License is distributed on an "AS IS" BASIS,

                  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

                  // See the License for the specific language governing permissions and

                  // limitations under the License.


                  package zoekt


                  import (
                  	"context"
                  	"fmt"
                  	"log"
                  	"regexp/syntax"
                  	"sort"
                  	"strings"
                  	"time"

                  	enry_data "github.com/go-enry/go-enry/v2/data"
                  	"github.com/grafana/regexp"

                  	"github.com/sourcegraph/zoekt/query"
                  )


                  // simplifyMultiRepo takes a query and a predicate. It returns Const(true) if all

                  // repository names fulfill the predicate, Const(false) if none of them do, and q

                  // otherwise.

                  func (d *indexData) simplifyMultiRepo(q query.Q, predicate func(*Repository) bool) query.Q {
                  	count := 0
                  	alive := len(d.repoMetaData)
                  	for i := range d.repoMetaData {
                  		if d.repoMetaData[i].Tombstone {
                  			alive--
                  		} else if predicate(&d.repoMetaData[i]) {
                  			count++
                  		}
                  	}
                  	if count == alive {
                  		return &query.Const{Value: true}
                  	}
                  	if count > 0 {
                  		return q
                  	}
                  	return &query.Const{Value: false}
                  }


                  func (d *indexData) simplify(in query.Q) query.Q {
                  	eval := query.Map(in, func(q query.Q) query.Q {
                  		switch r := q.(type) {
                  		case *query.Repo:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.RepoRegexp:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Regexp.MatchString(repo.Name)
                  			})
                  		case *query.BranchesRepos:
                  			for i := range d.repoMetaData {
                  				for _, br := range r.List {
                  					if br.Repos.Contains(d.repoMetaData[i].ID) {
                  						return q
                  					}
                  				}
                  			}
                  			return &query.Const{Value: false}
                  		case *query.RepoSet:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Set[repo.Name]
                  			})
                  		case *query.RepoIDs:
                  			return d.simplifyMultiRepo(q, func(repo *Repository) bool {
                  				return r.Repos.Contains(repo.ID)
                  			})
                  		case *query.Language:
                  			_, has := d.metaData.LanguageMap[r.Language]
                  			if !has && d.metaData.IndexFeatureVersion < 12 {
                  				// For index files that haven't been re-indexed by go-enry,
                  				// fall back to file-based matching and continue even if this
                  				// repo doesn't have the specific language present.
                  				extsForLang := enry_data.ExtensionsByLanguage[r.Language]
                  				if extsForLang != nil {
                  					extFrags := make([]string, 0, len(extsForLang))
                  					for _, ext := range extsForLang {
                  						extFrags = append(extFrags, regexp.QuoteMeta(ext))
                  					}
                  					if len(extFrags) > 0 {
                  						pattern := fmt.Sprintf("(?i)(%s)$", strings.Join(extFrags, "|"))
                  						// inlined copy of query.regexpQuery
                  						re, err := syntax.Parse(pattern, syntax.Perl)
                  						if err != nil {
                  							return &query.Const{Value: false}
                  						}
                  						if re.Op == syntax.OpLiteral {
                  							return &query.Substring{
                  								Pattern:  string(re.Rune),
                  								FileName: true,
                  							}
                  						}
                  						return &query.Regexp{
                  							Regexp:   re,
                  							FileName: true,
                  						}
                  					}
                  				}
                  			}
                  			if !has {
                  				return &query.Const{Value: false}
                  			}
                  		}
                  		return q
                  	})
                  	return query.Simplify(eval)
                  }


                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  }


                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  }


                  func addRepo(res *SearchResult, repo *Repository) {
                  	if res.RepoURLs == nil {
                  		res.RepoURLs = map[string]string{}
                  	}
                  	res.RepoURLs[repo.Name] = repo.FileURLTemplate

                  	if res.LineFragments == nil {
                  		res.LineFragments = map[string]string{}
                  	}
                  	res.LineFragments[repo.Name] = repo.LineFragmentTemplate
                  }


                  // Gather matches from this document. The matches are returned in document

                  // order and are non-overlapping. All filename and content matches are

                  // returned, with filename matches first.

                  //

                  // If `merge` is set, overlapping and adjacent matches will be merged

                  // into a single match. Otherwise, overlapping matches will be removed,

                  // but adjacent matches will remain.

                  func (d *indexData) gatherMatches(nextDoc uint32, mt matchTree, known map[matchTree]bool, merge bool) []*candidateMatch {
                  	var cands []*candidateMatch
                  	visitMatches(mt, known, 1, func(mt matchTree, scoreWeight float64) {
                  		if smt, ok := mt.(*substrMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.current)...)
                  		}
                  		if rmt, ok := mt.(*regexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if rmt, ok := mt.(*wordMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, rmt.found)...)
                  		}
                  		if smt, ok := mt.(*symbolRegexpMatchTree); ok {
                  			cands = append(cands, setScoreWeight(scoreWeight, smt.found)...)
                  		}
                  	})

                  	// If we found no candidate matches at all, assume there must have been a match on filename.
                  	if len(cands) == 0 {
                  		nm := d.fileName(nextDoc)
                  		return []*candidateMatch{{
                  			caseSensitive: false,
                  			fileName:      true,
                  			substrBytes:   nm,
                  			substrLowered: nm,
                  			file:          nextDoc,
                  			runeOffset:    0,
                  			byteOffset:    0,
                  			byteMatchSz:   uint32(len(nm)),
                  		}}
                  	}

                  	sort.Sort((sortByOffsetSlice)(cands))
                  	res := cands[:0]
                  	mergeRun := 1
                  	for i, c := range cands {
                  		if i == 0 {
                  			res = append(res, c)
                  			continue
                  		}

                  		last := res[len(res)-1]

                  		// Never compare filename and content matches
                  		if last.fileName != c.fileName {
                  			res = append(res, c)
                  			continue
                  		}

                  		if merge {
                  			// Merge adjacent candidates. This guarantees that the matches
                  			// are non-overlapping.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			end := c.byteOffset + c.byteMatchSz
                  			if lastEnd >= c.byteOffset {
                  				mergeRun++
                  				// Average out the score across the merged candidates. Only do it if
                  				// we are boosting to avoid floating point funkiness in the normal
                  				// case.
                  				if !(epsilonEqualsOne(last.scoreWeight) && epsilonEqualsOne(c.scoreWeight)) {
                  					last.scoreWeight = ((last.scoreWeight * float64(mergeRun-1)) + c.scoreWeight) / float64(mergeRun)
                  				}

                  				// latest candidate goes further, update our end
                  				if end > lastEnd {
                  					last.byteMatchSz = end - last.byteOffset
                  				}

                  				continue
                  			} else {
                  				mergeRun = 1
                  			}
                  		} else {
                  			// Remove overlapping candidates. This guarantees that the matches
                  			// are non-overlapping, but also preserves expected match counts.
                  			lastEnd := last.byteOffset + last.byteMatchSz
                  			if lastEnd > c.byteOffset {
                  				continue
                  			}
                  		}

                  		res = append(res, c)
                  	}
                  	return res
                  }


                  type sortByOffsetSlice []*candidateMatch


                  func (m sortByOffsetSlice) Len() int      { return len(m) }

                  func (m sortByOffsetSlice) Swap(i, j int) { m[i], m[j] = m[j], m[i] }

                  func (m sortByOffsetSlice) Less(i, j int) bool {
                  	// Sort all filename matches to the start
                  	if m[i].fileName != m[j].fileName {
                  		return m[i].fileName
                  	}

                  	if m[i].byteOffset == m[j].byteOffset { // tie break if same offset
                  		// Prefer longer candidates if starting at same position
                  		return m[i].byteMatchSz > m[j].byteMatchSz
                  	}
                  	return m[i].byteOffset < m[j].byteOffset
                  }


                  // setScoreWeight is a helper used by gatherMatches to set the weight based on

                  // the score weight of the matchTree.

                  func setScoreWeight(scoreWeight float64, cm []*candidateMatch) []*candidateMatch {
                  	for _, m := range cm {
                  		m.scoreWeight = scoreWeight
                  	}
                  	return cm
                  }


                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  }


                  // gatherBranches returns a list of branch names taking into account any branch

                  // filters in the query. If the query contains a branch filter, it returns all

                  // branches containing the docID and matching the branch filter. Otherwise, it

                  // returns all branches containing docID.

                  func (d *indexData) gatherBranches(docID uint32, mt matchTree, known map[matchTree]bool) []string {
                  	var mask uint64
                  	visitMatchAtoms(mt, known, func(mt matchTree) {
                  		bq, ok := mt.(*branchQueryMatchTree)
                  		if !ok {
                  			return
                  		}

                  		mask = mask | bq.branchMask()
                  	})

                  	if mask == 0 {
                  		mask = d.fileBranchMasks[docID]
                  	}

                  	var branches []string
                  	id := uint32(1)
                  	branchNames := d.branchNames[d.repos[docID]]
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			branches = append(branches, branchNames[uint(id)])
                  		}
                  		id <<= 1
                  		mask >>= 1
                  	}

                  	return branches
                  }


                  func (d *indexData) List(ctx context.Context, q query.Q, opts *ListOptions) (rl *RepoList, err error) {
                  	var include func(rle *RepoListEntry) bool

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok {
                  		if !c.Value {
                  			return &RepoList{}, nil
                  		}
                  		include = func(rle *RepoListEntry) bool {
                  			return true
                  		}
                  	} else {
                  		sr, err := d.Search(ctx, q, &SearchOptions{
                  			ShardRepoMaxMatchCount: 1,
                  		})
                  		if err != nil {
                  			return nil, err
                  		}

                  		foundRepos := make(map[string]struct{}, len(sr.Files))
                  		for _, file := range sr.Files {
                  			foundRepos[file.Repository] = struct{}{}
                  		}

                  		include = func(rle *RepoListEntry) bool {
                  			_, ok := foundRepos[rle.Repository.Name]
                  			return ok
                  		}
                  	}

                  	var l RepoList

                  	field, err := opts.GetField()
                  	if err != nil {
                  		return nil, err
                  	}
                  	switch field {
                  	case RepoListFieldRepos:
                  		l.Repos = make([]*RepoListEntry, 0, len(d.repoListEntry))
                  	case RepoListFieldReposMap:
                  		l.ReposMap = make(ReposMap, len(d.repoListEntry))
                  	}

                  	for i := range d.repoListEntry {
                  		if d.repoMetaData[i].Tombstone {
                  			continue
                  		}
                  		rle := &d.repoListEntry[i]
                  		if !include(rle) {
                  			continue
                  		}

                  		l.Stats.Add(&rle.Stats)

                  		// Backwards compat for when ID is missing
                  		if rle.Repository.ID == 0 {
                  			l.Repos = append(l.Repos, rle)
                  			continue
                  		}

                  		switch field {
                  		case RepoListFieldRepos:
                  			l.Repos = append(l.Repos, rle)
                  		case RepoListFieldReposMap:
                  			l.ReposMap[rle.Repository.ID] = MinimalRepoListEntry{
                  				HasSymbols:    rle.Repository.HasSymbols,
                  				Branches:      rle.Repository.Branches,
                  				IndexTimeUnix: rle.IndexMetadata.IndexTime.Unix(),
                  			}
                  		}

                  	}

                  	// Only one of these fields is populated and in all cases the size of that
                  	// field is the number of Repos in this shard.
                  	l.Stats.Repos = len(l.Repos) + len(l.ReposMap)

                  	return &l, nil
                  }


                  // regexpToMatchTreeRecursive converts a regular expression to a matchTree mt. If

                  // mt is equivalent to the input r, isEqual = true and the matchTree can be used

                  // in place of the regex r. If singleLine = true, then the matchTree and all

                  // its children only match terms on the same line. singleLine is used during

                  // recursion to decide whether to return an andLineMatchTree (singleLine = true)

                  // or a andMatchTree (singleLine = false).

                  func (d *indexData) regexpToMatchTreeRecursive(r *syntax.Regexp, minTextSize int, fileName bool, caseSensitive bool) (mt matchTree, isEqual bool, singleLine bool, err error) {
                  	// TODO - we could perhaps transform Begin/EndText in '/n'?
                  	// TODO - we could perhaps transform CharClass in (OrQuery )
                  	// if there are just a few runes, and part of a OpConcat?
                  	switch r.Op {
                  	case syntax.OpLiteral:
                  		s := string(r.Rune)
                  		if len(s) >= minTextSize {
                  			ignoreCase := syntax.FoldCase == (r.Flags & syntax.FoldCase)
                  			mt, err := d.newSubstringMatchTree(&query.Substring{Pattern: s, FileName: fileName, CaseSensitive: !ignoreCase && caseSensitive})
                  			return mt, true, !strings.Contains(s, "/n"), err
                  		}
                  	case syntax.OpCapture:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpPlus:
                  		return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)

                  	case syntax.OpRepeat:
                  		if r.Min == 1 {
                  			return d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  		} else if r.Min > 1 {
                  			// (x){2,} can't be expressed precisely by the matchTree
                  			mt, _, singleLine, err := d.regexpToMatchTreeRecursive(r.Sub[0], minTextSize, fileName, caseSensitive)
                  			return mt, false, singleLine, err
                  		}
                  	case syntax.OpConcat, syntax.OpAlternate:
                  		var qs []matchTree
                  		isEq := true
                  		singleLine = true
                  		for _, sr := range r.Sub {
                  			if sq, subIsEq, subSingleLine, err := d.regexpToMatchTreeRecursive(sr, minTextSize, fileName, caseSensitive); sq != nil {
                  				if err != nil {
                  					return nil, false, false, err
                  				}
                  				isEq = isEq && subIsEq
                  				singleLine = singleLine && subSingleLine
                  				qs = append(qs, sq)
                  			}
                  		}
                  		if r.Op == syntax.OpConcat {
                  			if len(qs) > 1 {
                  				isEq = false
                  			}
                  			newQs := make([]matchTree, 0, len(qs))
                  			for _, q := range qs {
                  				if _, ok := q.(*bruteForceMatchTree); ok {
                  					continue
                  				}
                  				newQs = append(newQs, q)
                  			}
                  			if len(newQs) == 1 {
                  				return newQs[0], isEq, singleLine, nil
                  			}
                  			if len(newQs) == 0 {
                  				return &bruteForceMatchTree{}, isEq, singleLine, nil
                  			}
                  			if singleLine {
                  				return &andLineMatchTree{andMatchTree{children: newQs}}, isEq, singleLine, nil
                  			}
                  			return &andMatchTree{newQs}, isEq, singleLine, nil
                  		}
                  		for _, q := range qs {
                  			if _, ok := q.(*bruteForceMatchTree); ok {
                  				return q, isEq, false, nil
                  			}
                  		}
                  		if len(qs) == 0 {
                  			return &noMatchTree{Why: "const"}, isEq, false, nil
                  		}
                  		return &orMatchTree{qs}, isEq, false, nil
                  	case syntax.OpStar:
                  		if r.Sub[0].Op == syntax.OpAnyCharNotNL {
                  			return &bruteForceMatchTree{}, false, true, nil
                  		}
                  	}
                  	return &bruteForceMatchTree{}, false, false, nil
                  }


                  type timer struct {
                  	last time.Time
                  }


                  func newTimer() *timer {
                  	return &timer{
                  		last: time.Now(),
                  	}
                  }


                  func (t *timer) Elapsed() time.Duration {
                  	now := time.Now()
                  	d := now.Sub(t.last)
                  	t.last = now
                  	return d
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: See zoekt repo find location
                  of tensor function"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 288895
        content:
          mimeType: text/event-stream
          size: 288895
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, here's a high-level overview:\n\nThe code appears to be part of the zoekt project, which is a fast text search engine intended for source code. The provided files contain various functions and types related to indexing, searching, and evaluating queries against the indexed data.\n\nThe main components that can be deduced from the given context are:\n\n1. `zoekt` package: This is the main package that houses the core functionality of the zoekt search engine.\n2. `SearchOptions` and `SearchResult` types: These structs represent the options and result data for performing searches.\n3. `indexData` struct: This struct holds the indexed data and provides methods for searching, simplifying queries, and converting regular expressions to match trees.\n4. `matchTree` interface and related implementations: These types represent different ways of matching patterns against the indexed data, such as substring matching, regular expression matching, and more.\n5. Helper functions for gathering matches, sorting, truncating, and scoring results.\n\nBased on the given context, it seems that the zoekt project does not contain any specific function named \"tensor\". However, it provides the necessary infrastructure for indexing and searching source code, which could potentially be used to search for tensor-related functions or code snippets.\n\nTo better understand how to find the location of a specific function (e.g., tensor) within the zoekt codebase or the indexed data, I would need additional context or information about the project structure, indexing process, and how the search functionality is intended to be used.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:50 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:45.842Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d27600cc01fedea7715d0c15e8a0b33d
      _order: 0
      cache: {}
      request:
        bodySize: 17526
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-3b55d464208a4fda1c89d40e40c534f9-e3c21877bab787a9-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Explain the logic in
                  src/agent.go, particularly how agents interact with ranking"
            model: anthropic/claude-3-sonnet-20240229
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 60656
        content:
          mimeType: text/event-stream
          size: 60656
          text: >+
            event: completion

            data: {"completion":"Unfortunately, the provided code context does not contain any information about `src/agent.go` or how agents interact with ranking. The provided context is only about limiting the number of displayed file matches and line matches based on the specified search options.\n\nTo explain the logic in `src/agent.go` and how agents interact with ranking, I would need access to that specific file as well as any other relevant files that contain code related to agents and ranking logic.\n\nWithout having access to that code, I cannot provide a meaningful explanation. Please provide the necessary code context related to `src/agent.go` and the ranking logic involving agents, and I'll be happy to analyze and explain it.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:58 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:56.386Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 8d1911b93b41993888a5ee51ec4d8564
      _order: 0
      cache: {}
      request:
        bodySize: 5060
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-91df5467c628c41242c13c2a957a9023-e9829cb9ea10d588-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: What does this repo do??"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 205317
        content:
          mimeType: text/event-stream
          size: 205317
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, this appears to be a part of the `zoekt` package, which seems to be a search-related functionality. The key points I can gather from the code are:\n\n1. `SortAndTruncateFiles`: This is a convenience function that takes a list of `FileMatch` instances, sorts them, and then truncates the list based on the provided `SearchOptions`.\n\n2. `DisplayTruncator`: This is a stateful function that enforces document and match display limits by truncating and mutating the input `FileMatch` list. It returns a new list with the truncation applied, along with a boolean indicating if there are more results that could be displayed.\n\n3. `NewDisplayTruncator`: This function creates a `DisplayTruncator` instance based on the provided `SearchOptions`. If there are no limits to enforce, it returns a no-op function.\n\n4. `limitMatches`: This function is used by `DisplayTruncator` to limit the number of matches (either chunk matches or line matches) in the `FileMatch` instances.\n\nWithout additional context, it's difficult to determine the exact purpose of this codebase. It seems to be dealing with search-related functionality, potentially for a code search or content search application, but more information would be needed to fully understand the context and overall goals of this code.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:02 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:01.333Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 7da620eca0779161892b671e6b40c592
      _order: 0
      cache: {}
      request:
        bodySize: 10348
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-2b4fdec53c71403f7f8680d159ddf634-9277083ed02fd542-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: describe my code"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 291467
        content:
          mimeType: text/event-stream
          size: 291467
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, this appears to be part of the Zoekt codebase, which is a fast text search engine for source code. The code is written in Go and focuses on the functionality related to limiting and truncating search results based on specified options.\n\nThe main components are:\n\n1. `SortAndTruncateFiles`: A convenience function that sorts the search results and then truncates them based on the provided search options.\n\n2. `DisplayTruncator`: A stateful function that enforces the document and match display limits by truncating and mutating the search results. It returns a boolean indicating whether there are more results to process.\n\n3. `NewDisplayTruncator`: A function that creates a `DisplayTruncator` based on the search options, returning a boolean indicating whether the truncation limits need to be enforced.\n\n4. `limitMatches`: A function that limits the number of matches in the search results, either by limiting the number of chunk matches or line matches, depending on the search options.\n\n5. `limitChunkMatches`: A function that limits the number of chunk matches in a `FileMatch` by truncating the content, ranges, and symbol information.\n\n6. `limitLineMatches`: A function that limits the number of line matches in a `FileMatch` by truncating the line fragments.\n\nThis code appears to be responsible for managing the display of search results, ensuring that the number of documents and matches shown to the user is limited based on the specified options. The truncation logic is designed to provide a meaningful and manageable set of results to the user.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:05 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:04.613Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 2344db82884cd0b6848529e300cdebd1
      _order: 0
      cache: {}
      request:
        bodySize: 23990
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-c0007fd860daeef2a51d160bd3c1c82c-28d8453e27b4a1ef-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: @zoekt describe my code"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 250865
        content:
          mimeType: text/event-stream
          size: 250865
          text: >+
            event: completion

            data: {"completion":"Based on the code provided, this appears to be a part of a search engine implementation called `zoekt`. The codebase is focused on handling sharded search, which allows the search to be distributed across multiple shards or partitions of the search index.\n\nThe key components and functionality include:\n\n1. `shards_test.go`: This file contains test cases for the sharded search functionality, including testing different search options like `useDocumentRanks`.\n\n2. `eval.go`: This file contains the implementation of the `indexData` struct, which is responsible for searching the index and returning the search results. It includes features like filtering, ranking, and handling of repository metadata.\n\n3. `eval_test.go`: This file contains tests for the `typeRepoSearcher` component, which is responsible for evaluating search queries that include repository type filters.\n\n4. `shards.go`: This file likely contains the implementation of the `shardedSearcher` and related functionality for managing the sharded search process.\n\n5. Utility functions and structs, such as `rankSearcher`, `crashSearcher`, and `createMockSearchResult`, which are used for testing and mocking purposes.\n\nOverall, the codebase appears to be focused on building a scalable and efficient search engine that can handle large amounts of data by partitioning the index into multiple shards. The tests cover various aspects of the search functionality, including handling of edge cases and different search options.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:10 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:09.748Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 26fb5c08ae4ff439d2aa97ca823170ef
      _order: 0
      cache: {}
      request:
        bodySize: 7796
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-af776b126c019bfaa23cdac2025a5598-f0bdd2f1d73a878d-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestSendByRepository(t *testing.T) {
                  	wantStats := zoekt.Stats{}
                  	wantStats.ShardsScanned = 1

                  	// n1, n2, n3 are the number of file matches for each of the 3 repositories in this
                  	// test.
                  	f := func(n1, n2, n3 uint8) bool {
                  		sr := createMockSearchResult(n1, n2, n3, wantStats)

                  		mock := &mockSender{}
                  		sendByRepository(sr, &zoekt.SearchOptions{}, mock)

                  		if diff := cmp.Diff(wantStats, mock.stats); diff != "" {
                  			t.Logf("-want,+got/n%s", diff)
                  			return false
                  		}

                  		nonZero := 0
                  		for _, l := range []uint8{n1, n2, n3} {
                  			if l > 0 {
                  				nonZero++
                  			}
                  		}
                  		if l := len(mock.files); l != nonZero {
                  			t.Logf("wanted results from %d repositores, got %d", nonZero, l)
                  			return false
                  		}

                  		gotTotal := 0
                  		for _, fs := range mock.files {
                  			gotTotal += len(fs)
                  		}
                  		wantTotal := int(n1) + int(n2) + int(n3)
                  		if gotTotal != wantTotal {
                  			t.Logf("wanted %d file matches, got %d", wantTotal, gotTotal)
                  			return false
                  		}

                  		for _, fs := range mock.files {
                  			if len(fs) == 0 {
                  				t.Logf("got search result with 0 file matches after split")
                  				return false
                  			}
                  		}
                  		return true
                  	}

                  	if err := quick.Check(f, nil); err != nil {
                  		t.Error(err)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file grpc/chunk/chunk.go:

                  ```go

                  func SendAll[T proto.Message](sendFunc func([]T) error, items ...T) error {
                  	c := New(sendFunc)

                  	err := c.Send(items...)
                  	if err != nil {
                  		return err
                  	}

                  	return c.Flush()
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: is my code base clean?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 289455
        content:
          mimeType: text/event-stream
          size: 289455
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, here's a high-level overview:\n\n1. The codebase seems to be related to a search engine, potentially for source code. This is evident from the README.md file, which mentions Zoekt, a fast text search engine intended for use with source code.\n\n2. The codebase includes various components, such as a CLI tool, an indexing server, and a web server. The indexing server is responsible for fetching, indexing, and mirroring repositories from sources like GitHub and Gitiles.\n\n3. The codebase also includes some Sourcegraph-specific changes, such as a custom indexing server, API exposure, and support for efficient repository specification and symbol/ctag data in search results.\n\n4. The code in the `grpc/chunk/chunk.go` file appears to be a utility function for sending data in chunks, likely to optimize network communication.\n\nWithout additional context, it's difficult to make a definitive assessment of the code's cleanliness. The provided code snippets seem to be well-structured and follow standard Go programming practices. However, to better evaluate the codebase's overall cleanliness, I would need more information, such as:\n\n1. The overall project structure and organization.\n2. Coding conventions and adherence to best practices.\n3. The presence of automated tests and the overall test coverage.\n4. The quality of the documentation and comments.\n5. The overall maintainability and ease of understanding the codebase.\n\nWith more context, I could provide a more detailed assessment of the codebase's cleanliness and make recommendations for potential improvements.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:16 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:15.359Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: b2c82fead204229329daf8b8db608ab9
      _order: 0
      cache: {}
      request:
        bodySize: 10238
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-65f8fb4beb64c9d49c85ee249407dbc7-f0ea4a840a8b3a60-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Can you look through the
                  files and identify the conflicting packages that may be
                  causing this?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 269673
        content:
          mimeType: text/event-stream
          size: 269673
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, it appears that this is a part of the Zoekt codebase, which is a fast text search engine for source code. The code in `limit.go` seems to be responsible for handling the display limits of search results, specifically limiting the number of documents and matches that are returned.\n\nThe main functions in this file are:\n\n1. `SortAndTruncateFiles`: This function takes a slice of `FileMatch` objects, sorts them, and then truncates the slice based on the provided `SearchOptions`.\n\n2. `NewDisplayTruncator`: This function returns a `DisplayTruncator` function that enforces the document and match display limits specified in the `SearchOptions`. The `DisplayTruncator` function is then used to truncate the `FileMatch` slice.\n\n3. `limitMatches`: This function is used by `NewDisplayTruncator` to limit the number of matches in each `FileMatch` object, based on the specified limits.\n\n4. `limitChunkMatches` and `limitLineMatches`: These functions are used by `limitMatches` to limit the number of chunk matches and line matches, respectively, in each `FileMatch` object.\n\nFrom the provided code, there does not seem to be any indication of conflicting packages. The code appears to be self-contained within the Zoekt codebase, without any obvious dependencies or conflicts.\n\nIf you have any specific concerns or issues you would like me to investigate further, please provide me with more context or details, and I'll be happy to assist you.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:20 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:19.223Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: bae751491de5acbdf9e5430b6ad63381
      _order: 0
      cache: {}
      request:
        bodySize: 10380
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-74832fe6466604461ca7745c5af64800-2abb3d483a613729-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Mind taking a second look at
                  the file? @limit.go"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 225515
        content:
          mimeType: text/event-stream
          size: 225515
          text: >+
            event: completion

            data: {"completion":"Sure, let's take a high-level look at the `limit.go` file:\n\nThe `limit.go` file contains functions related to limiting the search results for display purposes. The main functions are:\n\n1. `SortAndTruncateFiles`: This function takes a slice of `FileMatch` objects and sorts them, then truncates the result based on the provided `SearchOptions`.\n\n2. `NewDisplayTruncator`: This function creates a `DisplayTruncator` function that can be used to truncate the `FileMatch` objects based on the `SearchOptions`. The truncator enforces limits on the maximum number of documents and matches to display.\n\n3. `limitMatches`: This function is used by the `DisplayTruncator` to limit the number of matches within each `FileMatch` object, either by limiting the number of `ChunkMatches` or `LineMatches`.\n\n4. `limitChunkMatches`: This function is used to limit the number of `ChunkMatches` within a `FileMatch` object by truncating the `Ranges`, `SymbolInfo`, and `Content` fields.\n\n5. `limitLineMatches`: This function is used to limit the number of `LineMatches` within a `FileMatch` object by truncating the `LineFragments` field.\n\nThe main purpose of this file is to ensure that the search results are truncated to a reasonable size for display, based on the limits specified in the `SearchOptions`.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:24 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:23.194Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 179826710d5cbe9d2e922909e3ca48f0
      _order: 0
      cache: {}
      request:
        bodySize: 10186
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-85df8991e28a869af8fbc53f3c3f4c4c-8d9a819a57aa36d5-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Why does this project use
                  the MIT license?"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 173870
        content:
          mimeType: text/event-stream
          size: 173870
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, this appears to be a part of the Zoekt project, which is a fast text search engine for source code. The code seems to focus on the functionality related to limiting the number of search results displayed, such as limiting the number of documents and matches shown. \n\nThe `SortAndTruncateFiles` function is a convenience function that sorts the search results and then truncates them based on the search options. The `NewDisplayTruncator` function creates a function that enforces the document and match display limits specified in the `SearchOptions`.\n\nThe `limitChunkMatches` and `limitLineMatches` functions handle the actual truncation of the search results, ensuring that the content, ranges, and symbol information are properly adjusted.\n\nWithout any additional context about the project's licensing, I cannot provide a definitive answer as to why it uses the MIT license. The MIT license is a permissive open-source license that allows for commercial use, modification, distribution, and private use, while requiring the inclusion of the original copyright and license notices. However, more information about the project's history, goals, and community would be needed to determine the specific reasons for choosing the MIT license.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:27 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:26.516Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: dbfc52932ca760342243b8248b410280
      _order: 0
      cache: {}
      request:
        bodySize: 10191
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-643b9fcaf05abd5713930b0f4af76752-e06985749aeac87a-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: See zoekt repo find location
                  of tensor function"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 280157
        content:
          mimeType: text/event-stream
          size: 280157
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, here's a high-level overview:\n\nThe `zoekt` package contains code related to a fast text search engine, primarily designed for use with source code. The `limit.go` file contains functions for managing the display of search results, specifically handling the truncation and sorting of file matches based on search options.\n\nThe key functions in the provided context are:\n\n1. `SortAndTruncateFiles`: This function takes a list of `FileMatch` objects, sorts them, and then truncates the list based on the search options provided.\n\n2. `NewDisplayTruncator`: This function returns a `DisplayTruncator` function that can be used to enforce document and match display limits, truncating the list of `FileMatch` objects as needed.\n\n3. `limitChunkMatches` and `limitLineMatches`: These functions are used by the `NewDisplayTruncator` function to limit the number of chunk matches and line matches, respectively, in each `FileMatch` object.\n\nTo answer the question \"See zoekt repo find location of tensor function\", I would need additional context, specifically:\n\n1. Is the \"tensor function\" a reference to a specific function in the zoekt codebase?\n2. If so, where is this function located (which file, line number, etc.)?\n\nWithout more information about the specific \"tensor function\" you are looking for, I cannot provide a definitive answer based on the provided code context alone. Please provide additional details about the function you are trying to locate, and I'll be happy to assist further.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:30 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:29.891Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: aaf5ff5ea487605e072dacabec282a28
      _order: 0
      cache: {}
      request:
        bodySize: 17525
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-d0637859093a641f5f72bd0cb5e1bbc7-a8abe80b22d8d182-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 431
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: system
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Explain the logic in
                  src/agent.go, particularly how agents interact with ranking"
            model: anthropic/claude-3-haiku-20240307
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: api-version
            value: "1"
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?api-version=1&client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 247905
        content:
          mimeType: text/event-stream
          size: 247905
          text: >+
            event: completion

            data: {"completion":"Unfortunately, I do not have access to the `src/agent.go` file, as the provided code context is from the `limit.go` file. The `limit.go` file appears to contain functions related to sorting and truncating search results, specifically `FileMatch` objects, based on certain search options and limits. The main functions in this file are:\n\n1. `SortAndTruncateFiles`: This function takes a slice of `FileMatch` objects and sorts them, then applies truncation based on the provided `SearchOptions`.\n\n2. `NewDisplayTruncator`: This function returns a `DisplayTruncator` function, which is used to enforce document and match display limits by truncating the provided `FileMatch` slice.\n\n3. `limitMatches`: This function is used by `NewDisplayTruncator` to limit the number of matches (either chunk matches or line matches) in each `FileMatch` object, based on the provided limit.\n\n4. `limitChunkMatches` and `limitLineMatches`: These functions are used by `limitMatches` to actually perform the truncation of chunk matches and line matches, respectively.\n\nWithout access to the `src/agent.go` file, I cannot provide a detailed explanation of the agent interaction and ranking logic. I would need more context from that file to answer your question fully. Please provide the contents of the `src/agent.go` file, or any other relevant files, so that I can better understand the overall system and provide a more comprehensive response.","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:36 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:35.641Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: e52920573ed295d810554f32ca625023
      _order: 0
      cache: {}
      request:
        bodySize: 1116
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-5bb4e867264865df29ae54a839b5f66c-41d6e49480a4fc78-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>The provided context are the code you need
                  and have access to. Do not make any assumptions. Give me a
                  high-level overview based on the given context first before
                  asking for additional context to answer the following
                  question: @zoekt describe my code</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 69915
        content:
          mimeType: text/event-stream
          size: 69915
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ezoekt\u003c/value\u003e\n\u003cvariants\u003esearch search-engine indexer\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode\u003c/value\u003e\n\u003cvariants\u003ecodebase source-code program\u003c/variants\u003e\n\u003cweight\u003e0.9\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eoverview\u003c/value\u003e\n\u003cvariants\u003esummary high-level-description\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003econtext\u003c/value\u003e\n\u003cvariants\u003eprovided-context given-context\u003c/variants\u003e\n\u003cweight\u003e0.7\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:06 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:05.663Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: a4eedee3f174838eb837335de7c41c16
      _order: 0
      cache: {}
      request:
        bodySize: 1115
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-79e9b6ec1d46685c3f8f32b6f1df2ba6-3098ed7a9d5e26a7-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>The provided context are the code you need
                  and have access to. Do not make any assumptions. Give me a
                  high-level overview based on the given context first before
                  asking for additional context to answer the following
                  question: is my code base clean?</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 66965
        content:
          mimeType: text/event-stream
          size: 66965
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003ecode\u003c/value\u003e\n\u003cvariants\u003esource code codebase repository\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003econtext\u003c/value\u003e\n\u003cvariants\u003ebackground information\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eoverview\u003c/value\u003e\n\u003cvariants\u003esummary high-level\u003c/variants\u003e\n\u003cweight\u003e0.9\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eclean\u003c/value\u003e\n\u003cvariants\u003emaintainable organized well-structured\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:16 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:15.317Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d368b1e485b011efea5c611c629ce31e
      _order: 0
      cache: {}
      request:
        bodySize: 1173
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-3b55d464208a4fda1c89d40e40c534f9-68c8ffedd259bc7a-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            fast: true
            maxTokensToSample: 400
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: "You are helping the user search over a codebase. List some filename
                  fragments that would match files relevant to read to answer
                  the user's query. Present your results in a *single* XML list
                  in the following format: <keywords><keyword><value>a single
                  keyword</value><variants>a space separated list of synonyms
                  and variants of the keyword, including acronyms,
                  abbreviations, and expansions</variants><weight>a numerical
                  weight between 0.0 and 1.0 that indicates the importance of
                  the keyword</weight></keyword></keywords>. Here is the user
                  query: <userQuery>The provided context are the code you need
                  and have access to. Do not make any assumptions. Give me a
                  high-level overview based on the given context first before
                  asking for additional context to answer the following
                  question: Explain the logic in src/agent.go, particularly how
                  agents interact with ranking</userQuery>"
              - speaker: assistant
            temperature: 0
            topK: 1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 62156
        content:
          mimeType: text/event-stream
          size: 62156
          text: >+
            event: completion

            data: {"completion":"\u003ckeywords\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eagent\u003c/value\u003e\n\u003cvariants\u003eagent agent.go agents\u003c/variants\u003e\n\u003cweight\u003e1.0\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eranking\u003c/value\u003e\n\u003cvariants\u003erank ranking ranks\u003c/variants\u003e\n\u003cweight\u003e0.9\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003elogic\u003c/value\u003e\n\u003cvariants\u003elogic logics\u003c/variants\u003e\n\u003cweight\u003e0.8\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003ckeyword\u003e\n\u003cvalue\u003eoverview\u003c/value\u003e\n\u003cvariants\u003eoverview overview.md\u003c/variants\u003e\n\u003cweight\u003e0.7\u003c/weight\u003e\n\u003c/keyword\u003e\n\u003c/keywords\u003e","stopReason":"end_turn"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:09:55 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:09:54.687Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: a5dfe085ca74d4c55328059c9d18cbb9
      _order: 0
      cache: {}
      request:
        bodySize: 5155
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-3920ebf132d2528a021ea41330b97200-cfa84d51f2518d0c-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: What does this repo do??"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 54935
        content:
          mimeType: text/event-stream
          size: 54935
          text: >+
            event: completion

            data: {"completion":"Based on the provided code, it looks like this codebase is related to a search functionality. The `SortAndTruncateFiles` function takes a list of `FileMatch` structs, sorts them, and then truncates them based on search options. The `DisplayTruncator` function enforces limits on displaying document and match counts. The `NewDisplayTruncator` function creates a truncator based on search options. The `limitChunkMatches` and `limitLineMatches` functions limit the number of chunk matches and line matches in a `FileMatch` struct, respectively. Overall, it seems like this repository is handling search functionality and displaying search results in an organized and limited way.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:39 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:39.216Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 2c260d3331cb73f63812514054626e39
      _order: 0
      cache: {}
      request:
        bodySize: 10443
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-bcf9e9e53c22caa59563bc2b5ab621e7-034976520f4dd399-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: describe my code"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 52954
        content:
          mimeType: text/event-stream
          size: 52954
          text: >+
            event: completion

            data: {"completion":"Based on the provided code snippet, it appears to be a package related to the Zoekt search engine. The code contains functions for sorting and truncating files based on search options, enforcing display limits for documents and matches, and limiting the number of chunk matches or line matches in a file match. The code includes functions like `SortAndTruncateFiles`, `NewDisplayTruncator`, `limitMatches`, `limitChunkMatches`, and `limitLineMatches` which handle sorting, truncating, and limiting matches in the search results. Additionally, there are functions to handle different types of matches and enforce display limits for documents and matches in the search results.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:44 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:43.410Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: c985ea8777cc492f0e425824e5204b7d
      _order: 0
      cache: {}
      request:
        bodySize: 24085
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-840c4bf686cf17a2b54cdf6fb8f8e316-1164df5a87341211-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedStreamSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	var files []zoekt.FileMatch
                  	sender := zoekt.SenderFunc(func(result *zoekt.SearchResult) {
                  		files = append(files, result.Files...)
                  	})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 10 * time.Millisecond
                  	}
                  	if err := ss.StreamSearch(context.Background(), q, &opts, sender); err != nil {
                  		t.Fatal(err)
                  	}
                  	return files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func testShardedSearch(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch {
                  	ss := newShardedSearcher(1)
                  	searcher := searcherForTest(t, ib)
                  	ss.replace(map[string]zoekt.Searcher{"r1": searcher})

                  	opts := zoekt.SearchOptions{}
                  	if useDocumentRanks {
                  		opts.UseDocumentRanks = true
                  		opts.FlushWallTime = 50 * time.Millisecond
                  	}
                  	sres, _ := ss.Search(context.Background(), q, &opts)
                  	return sres.Files
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestRawQuerySearch(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, rawConfig map[string]string, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{Name: repo}
                  		r.RawConfig = rawConfig
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard("public", map[string]string{"public": "1"}, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})
                  	addShard("private_archived", map[string]string{"archived": "1"}, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("public_fork", map[string]string{"public": "1", "fork": "1"}, zoekt.Document{Name: "f3", Content: []byte("foo bar")})

                  	cases := []struct {
                  		pattern   string
                  		flags     query.RawConfig
                  		wantRepos []string
                  		wantFiles int
                  	}{
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic,
                  			wantRepos: []string{"public", "public_fork"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPublic | query.RcNoForks,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bar",
                  			flags:     query.RcOnlyForks,
                  			wantRepos: []string{"public_fork"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcNoArchived,
                  			wantRepos: []string{"public"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcNoForks,
                  			wantRepos: []string{"public", "private_archived"},
                  			wantFiles: 2,
                  		},
                  		{
                  			pattern:   "bas",
                  			flags:     query.RcOnlyArchived,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate,
                  			wantRepos: []string{"private_archived"},
                  			wantFiles: 1,
                  		},
                  		{
                  			pattern:   "foo",
                  			flags:     query.RcOnlyPrivate | query.RcNoArchived,
                  			wantRepos: []string{},
                  			wantFiles: 0,
                  		},
                  	}
                  	for _, c := range cases {
                  		t.Run(fmt.Sprintf("pattern:%s", c.pattern), func(t *testing.T) {
                  			q := query.NewAnd(&query.Substring{Pattern: c.pattern}, c.flags)

                  			sr, err := ss.Search(context.Background(), q, &zoekt.SearchOptions{})
                  			if err != nil {
                  				t.Fatal(err)
                  			}

                  			if got := len(sr.Files); got != c.wantFiles {
                  				t.Fatalf("wanted %d, got %d", c.wantFiles, got)
                  			}

                  			if c.wantFiles == 0 {
                  				return
                  			}

                  			gotRepos := make([]string, 0, len(sr.RepoURLs))
                  			for k := range sr.RepoURLs {
                  				gotRepos = append(gotRepos, k)
                  			}
                  			sort.Strings(gotRepos)
                  			sort.Strings(c.wantRepos)
                  			if d := cmp.Diff(c.wantRepos, gotRepos); d != "" {
                  				t.Fatalf("(-want, +got):/n%s", d)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestFileBasedSearch(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	c1 := []byte("I love bananas without skin")
                  	// -----------0123456789012345678901234567890123456789
                  	c2 := []byte("In Dutch, ananas means pineapple")
                  	// -----------0123456789012345678901234567890123456789
                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: c1},
                  		zoekt.Document{Name: "f2", Content: c2},
                  	)

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				matches := tt.testShardedSearch(t, &query.Substring{
                  					CaseSensitive: false,
                  					Pattern:       "ananas",
                  				}, b, useDocumentRanks)

                  				if len(matches) != 2 {
                  					t.Fatalf("got %v, want 2 matches", matches)
                  				}
                  				if matches[0].FileName != "f2" || matches[1].FileName != "f1" {
                  					t.Fatalf("got %v, want matches {f1,f2}", matches)
                  				}
                  				if matches[0].LineMatches[0].LineFragments[0].Offset != 10 || matches[1].LineMatches[0].LineFragments[0].Offset != 8 {
                  					t.Fatalf("got %#v, want offsets 10,8", matches)
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func mkSearchResult(n int, repoID uint32) *zoekt.SearchResult {
                  	if n == 0 {
                  		return &zoekt.SearchResult{}
                  	}
                  	fm := make([]zoekt.FileMatch, 0, n)
                  	for i := 0; i < n; i++ {
                  		fm = append(fm, zoekt.FileMatch{Repository: fmt.Sprintf("repo%d", repoID), RepositoryID: repoID})
                  	}

                  	return &zoekt.SearchResult{Files: fm, RepoURLs: map[string]string{fmt.Sprintf("repo%d", repoID): ""}}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func createMockSearchResult(n1, n2, n3 uint8, stats zoekt.Stats) *zoekt.SearchResult {
                  	sr := &zoekt.SearchResult{RepoURLs: make(map[string]string)}
                  	for i, n := range []uint8{n1, n2, n3} {
                  		if n == 0 {
                  			continue
                  		}
                  		tmp := mkSearchResult(int(n), uint32(i))
                  		sr.Files = append(sr.Files, tmp.Files...)
                  		for k := range tmp.RepoURLs {
                  			sr.RepoURLs[k] = ""
                  		}
                  	}
                  	sr.Stats = stats
                  	return sr
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func BenchmarkShardedSearch(b *testing.B) {
                  	ss := newShardedSearcher(int64(runtime.GOMAXPROCS(0)))

                  	filesPerRepo := 300
                  	repos := reposForTest(3000)
                  	var repoSetIDs []uint32

                  	shards := make(map[string]zoekt.Searcher, len(repos))
                  	for i, r := range repos {
                  		shards[r.Name] = testSearcherForRepo(b, r, filesPerRepo)
                  		if i%2 == 0 {
                  			repoSetIDs = append(repoSetIDs, r.ID)
                  		}
                  	}

                  	ss.replace(shards)

                  	ctx := context.Background()
                  	opts := &zoekt.SearchOptions{}

                  	needleSub := &query.Substring{Pattern: "needle"}
                  	haystackSub := &query.Substring{Pattern: "haystack"}
                  	helloworldSub := &query.Substring{Pattern: "helloworld"}
                  	haystackCap, err := query.Parse("hay(s(t))(a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	haystackNonCap, err := query.Parse("hay(?:s(?:t))(?:a)ck")
                  	if err != nil {
                  		b.Fatal(err)
                  	}

                  	setAnd := func(q query.Q) func() query.Q {
                  		return func() query.Q {
                  			return query.NewAnd(query.NewSingleBranchesRepos("head", repoSetIDs...), q)
                  		}
                  	}

                  	search := func(b *testing.B, q query.Q, wantFiles int) {
                  		b.Helper()

                  		res, err := ss.Search(ctx, q, opts)
                  		if err != nil {
                  			b.Fatalf("Search(%s): %v", q, err)
                  		}
                  		if have := len(res.Files); have != wantFiles {
                  			b.Fatalf("wrong number of file results. have=%d, want=%d", have, wantFiles)
                  		}
                  	}

                  	benchmarks := []struct {
                  		name      string
                  		q         func() query.Q
                  		wantFiles int
                  	}{
                  		{"substring all results", func() query.Q { return haystackSub }, len(repos) * filesPerRepo},
                  		{"substring no results", func() query.Q { return helloworldSub }, 0},
                  		{"substring some results", func() query.Q { return needleSub }, len(repos)},

                  		{"regexp all results capture", func() query.Q { return haystackCap }, len(repos) * filesPerRepo},
                  		{"regexp all results non-capture", func() query.Q { return haystackNonCap }, len(repos) * filesPerRepo},

                  		{"substring all results and repo set", setAnd(haystackSub), len(repoSetIDs) * filesPerRepo},
                  		{"substring some results and repo set", setAnd(needleSub), len(repoSetIDs)},
                  		{"substring no results and repo set", setAnd(helloworldSub), 0},
                  	}

                  	for _, bb := range benchmarks {
                  		b.Run(bb.name, func(b *testing.B) {
                  			b.ReportAllocs()

                  			for n := 0; n < b.N; n++ {
                  				q := bb.q()

                  				search(b, q, bb.wantFiles)
                  			}
                  		})
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *crashSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	panic("search")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) StreamSearch(ctx context.Context, q query.Q, opts *zoekt.SearchOptions, sender zoekt.Sender) (err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.StreamSearch", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	var stats zoekt.Stats
                  	defer func() {
                  		tr.LazyPrintf("stats: %+v", stats)
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return err
                  	}

                  	return s.Streamer.StreamSearch(ctx, q, opts, zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  		stats.Add(event.Stats)
                  		sender.Send(event)
                  	}))
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval.go:

                  ```go

                  func (s *typeRepoSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (sr *zoekt.SearchResult, err error) {
                  	tr, ctx := trace.New(ctx, "typeRepoSearcher.Search", "")
                  	tr.LazyLog(q, true)
                  	tr.LazyPrintf("opts: %+v", opts)
                  	defer func() {
                  		if sr != nil {
                  			tr.LazyPrintf("num files: %d", len(sr.Files))
                  			tr.LazyPrintf("stats: %+v", sr.Stats)
                  		}
                  		if err != nil {
                  			tr.LazyPrintf("error: %v", err)
                  			tr.SetError(err)
                  		}
                  		tr.Finish()
                  	}()

                  	q, err = s.eval(ctx, q)
                  	if err != nil {
                  		return nil, err
                  	}

                  	return s.Streamer.Search(ctx, q, opts)
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/eval_test.go:

                  ```go

                  func TestSearchTypeRepo(t *testing.T) {
                  	ss := newShardedSearcher(2)
                  	nextShardNum := 1
                  	addShard := func(docs ...zoekt.Document) {
                  		b := testIndexBuilder(t, &zoekt.Repository{ID: 1, Name: "reponame"}, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{fmt.Sprintf("key-%d", nextShardNum): shard})
                  		nextShardNum++
                  	}
                  	addShard(
                  		zoekt.Document{Name: "f1", Content: []byte("bla the needle")},
                  		zoekt.Document{Name: "f2", Content: []byte("another file another needle")})
                  	addShard(
                  		zoekt.Document{Name: "f3", Content: []byte("another shard")})

                  	searcher := &typeRepoSearcher{ss}
                  	search := func(q query.Q, o ...zoekt.SearchOptions) *zoekt.SearchResult {
                  		t.Helper()
                  		var opts zoekt.SearchOptions
                  		if len(o) > 0 {
                  			opts = o[0]
                  		}
                  		res, err := searcher.Search(context.Background(), q, &opts)
                  		if err != nil {
                  			t.Fatalf("Search(%s): %v", q, err)
                  		}
                  		return res
                  	}
                  	wantSingleMatch := func(res *zoekt.SearchResult, want string) {
                  		t.Helper()
                  		fmatches := res.Files
                  		if len(fmatches) != 1 || len(fmatches[0].LineMatches) != 1 {
                  			t.Fatalf("got %v, want 1 matches", fmatches)
                  		}
                  		got := fmt.Sprintf("%s:%d", fmatches[0].FileName, fmatches[0].LineMatches[0].LineFragments[0].Offset)
                  		if got != want {
                  			t.Errorf("1: got %s, want %s", got, want)
                  		}
                  	}

                  	// type filter matches in different file
                  	res := search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "bla"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches in same file. Do not include that result
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "needle"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in different file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f1", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// type filter matches path in same file
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "f2", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")

                  	// no match by content
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// no match by path
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "nope", FileName: true},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	if len(res.Files) != 0 {
                  		t.Fatalf("got %v, want 0 matches", len(res.Files))
                  	}

                  	// type filter matches in a different shard
                  	res = search(query.NewAnd(
                  		&query.Type{
                  			Type:  query.TypeRepo,
                  			Child: &query.Substring{Pattern: "another shard"},
                  		},
                  		&query.Substring{Pattern: "file"}))
                  	wantSingleMatch(res, "f2:8")
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (o *SearchOptions) SetDefaults() {
                  	if o.ShardMaxMatchCount == 0 {
                  		// We cap the total number of matches, so overly broad
                  		// searches don't crash the machine.
                  		o.ShardMaxMatchCount = 100000
                  	}
                  	if o.TotalMaxMatchCount == 0 {
                  		o.TotalMaxMatchCount = 10 * o.ShardMaxMatchCount
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file eval.go:

                  ```go

                  func (d *indexData) Search(ctx context.Context, q query.Q, opts *SearchOptions) (sr *SearchResult, err error) {
                  	timer := newTimer()

                  	copyOpts := *opts
                  	opts = &copyOpts
                  	opts.SetDefaults()

                  	var res SearchResult
                  	if len(d.fileNameIndex) == 0 {
                  		return &res, nil
                  	}

                  	select {
                  	case <-ctx.Done():
                  		res.Stats.ShardsSkipped++
                  		return &res, nil
                  	default:
                  	}

                  	q = d.simplify(q)
                  	if c, ok := q.(*query.Const); ok && !c.Value {
                  		return &res, nil
                  	}

                  	if opts.EstimateDocCount {
                  		res.Stats.ShardFilesConsidered = len(d.fileBranchMasks)
                  		return &res, nil
                  	}

                  	q = query.Map(q, query.ExpandFileContent)

                  	mt, err := d.newMatchTree(q, matchTreeOpt{})
                  	if err != nil {
                  		return nil, err
                  	}

                  	// Capture the costs of construction before pruning
                  	updateMatchTreeStats(mt, &res.Stats)

                  	mt, err = pruneMatchTree(mt)
                  	if err != nil {
                  		return nil, err
                  	}
                  	res.Stats.MatchTreeConstruction = timer.Elapsed()
                  	if mt == nil {
                  		res.Stats.ShardsSkippedFilter++
                  		return &res, nil
                  	}

                  	res.Stats.ShardsScanned++

                  	cp := &contentProvider{
                  		id:    d,
                  		stats: &res.Stats,
                  	}

                  	// Track the number of documents found in a repository for
                  	// ShardRepoMaxMatchCount
                  	var (
                  		lastRepoID     uint16
                  		repoMatchCount int
                  	)

                  	docCount := uint32(len(d.fileBranchMasks))
                  	lastDoc := int(-1)

                  nextFileMatch:
                  	for {
                  		canceled := false
                  		select {
                  		case <-ctx.Done():
                  			canceled = true
                  		default:
                  		}

                  		nextDoc := mt.nextDoc()
                  		if int(nextDoc) <= lastDoc {
                  			nextDoc = uint32(lastDoc + 1)
                  		}

                  		for ; nextDoc < docCount; nextDoc++ {
                  			repoID := d.repos[nextDoc]
                  			repoMetadata := &d.repoMetaData[repoID]

                  			// Skip tombstoned repositories
                  			if repoMetadata.Tombstone {
                  				continue
                  			}

                  			// Skip documents that are tombstoned
                  			if len(repoMetadata.FileTombstones) > 0 {
                  				if _, tombstoned := repoMetadata.FileTombstones[string(d.fileName(nextDoc))]; tombstoned {
                  					continue
                  				}
                  			}

                  			// Skip documents over ShardRepoMaxMatchCount if specified.
                  			if opts.ShardRepoMaxMatchCount > 0 {
                  				if repoMatchCount >= opts.ShardRepoMaxMatchCount && repoID == lastRepoID {
                  					res.Stats.FilesSkipped++
                  					continue
                  				}
                  			}

                  			break
                  		}

                  		if nextDoc >= docCount {
                  			break
                  		}

                  		lastDoc = int(nextDoc)

                  		// We track lastRepoID for ShardRepoMaxMatchCount
                  		if lastRepoID != d.repos[nextDoc] {
                  			lastRepoID = d.repos[nextDoc]
                  			repoMatchCount = 0
                  		}

                  		if canceled || (res.Stats.MatchCount >= opts.ShardMaxMatchCount && opts.ShardMaxMatchCount > 0) {
                  			res.Stats.FilesSkipped += int(docCount - nextDoc)
                  			break
                  		}

                  		res.Stats.FilesConsidered++
                  		mt.prepare(nextDoc)

                  		cp.setDocument(nextDoc)

                  		known := make(map[matchTree]bool)
                  		md := d.repoMetaData[d.repos[nextDoc]]

                  		for cost := costMin; cost <= costMax; cost++ {
                  			switch evalMatchTree(cp, cost, known, mt) {
                  			case matchesRequiresHigherCost:
                  				if cost == costMax {
                  					log.Panicf("did not decide. Repo %s, doc %d, known %v",
                  						md.Name, nextDoc, known)
                  				}
                  			case matchesFound:
                  				// could short-circuit now, but we want to run higher costs to
                  				// potentially find higher ranked matches.
                  			case matchesNone:
                  				continue nextFileMatch
                  			}
                  		}

                  		fileMatch := FileMatch{
                  			Repository:         md.Name,
                  			RepositoryID:       md.ID,
                  			RepositoryPriority: md.priority,
                  			FileName:           string(d.fileName(nextDoc)),
                  			Checksum:           d.getChecksum(nextDoc),
                  			Language:           d.languageMap[d.getLanguage(nextDoc)],
                  		}

                  		if s := d.subRepos[nextDoc]; s > 0 {
                  			if s >= uint32(len(d.subRepoPaths[d.repos[nextDoc]])) {
                  				log.Panicf("corrupt index: subrepo %d beyond %v", s, d.subRepoPaths)
                  			}
                  			path := d.subRepoPaths[d.repos[nextDoc]][s]
                  			fileMatch.SubRepositoryPath = path
                  			sr := md.SubRepoMap[path]
                  			fileMatch.SubRepositoryName = sr.Name
                  			if idx := d.branchIndex(nextDoc); idx >= 0 {
                  				fileMatch.Version = sr.Branches[idx].Version
                  			}
                  		} else {
                  			idx := d.branchIndex(nextDoc)
                  			if idx >= 0 {
                  				fileMatch.Version = md.Branches[idx].Version
                  			}
                  		}

                  		// Important invariant for performance: finalCands is sorted by offset and
                  		// non-overlapping. gatherMatches respects this invariant and all later
                  		// transformations respect this.
                  		shouldMergeMatches := !opts.ChunkMatches
                  		finalCands := d.gatherMatches(nextDoc, mt, known, shouldMergeMatches)

                  		if opts.ChunkMatches {
                  			fileMatch.ChunkMatches = cp.fillChunkMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		} else {
                  			fileMatch.LineMatches = cp.fillMatches(finalCands, opts.NumContextLines, fileMatch.Language, opts.DebugScore)
                  		}

                  		if opts.UseBM25Scoring {
                  			d.scoreFileUsingBM25(&fileMatch, nextDoc, finalCands, opts)
                  		} else {
                  			// Use the standard, non-experimental scoring method by default
                  			d.scoreFile(&fileMatch, nextDoc, mt, known, opts)
                  		}

                  		fileMatch.Branches = d.gatherBranches(nextDoc, mt, known)
                  		sortMatchesByScore(fileMatch.LineMatches)
                  		sortChunkMatchesByScore(fileMatch.ChunkMatches)
                  		if opts.Whole {
                  			fileMatch.Content = cp.data(false)
                  		}

                  		matchedChunkRanges := 0
                  		for _, cm := range fileMatch.ChunkMatches {
                  			matchedChunkRanges += len(cm.Ranges)
                  		}

                  		repoMatchCount += len(fileMatch.LineMatches)
                  		repoMatchCount += matchedChunkRanges

                  		if opts.DebugScore {
                  			fileMatch.Debug = fmt.Sprintf("score:%.2f <- %s", fileMatch.Score, fileMatch.Debug)
                  		}

                  		res.Files = append(res.Files, fileMatch)
                  		res.Stats.MatchCount += len(fileMatch.LineMatches)
                  		res.Stats.MatchCount += matchedChunkRanges
                  		res.Stats.FileCount++
                  	}

                  	for _, md := range d.repoMetaData {
                  		r := md
                  		addRepo(&res, &r)
                  		for _, v := range r.SubRepoMap {
                  			addRepo(&res, v)
                  		}
                  	}

                  	// Update stats based on work done during document search.
                  	updateMatchTreeStats(mt, &res.Stats)

                  	// If document ranking is enabled, then we can rank and truncate the files to save memory.
                  	if opts.UseDocumentRanks {
                  		res.Files = SortAndTruncateFiles(res.Files, opts)
                  	}

                  	res.Stats.MatchTreeSearch = timer.Elapsed()

                  	return &res, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file eval.go:
                  ```go
                  func (d *indexData) branchIndex(docID uint32) int {
                  	mask := d.fileBranchMasks[docID]
                  	idx := 0
                  	for mask != 0 {
                  		if mask&0x1 != 0 {
                  			return idx
                  		}
                  		idx++
                  		mask >>= 1
                  	}
                  	return -1
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: @zoekt describe my code"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 137498
        content:
          mimeType: text/event-stream
          size: 137498
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, the code appears to be part of a search functionality within an indexing system. The `Search` method of the `indexData` struct is responsible for executing a search query against the indexed data. \n\nHere are some key points inferred from the code:\n- The method handles searching for documents based on a provided query and search options.\n- It performs various optimizations and checks, such as skipping tombstoned repositories and documents.\n- It constructs a match tree to evaluate the query against the indexed data and retrieve matching documents.\n- It gathers and processes matching file information, including metadata, content, matches, branches, scores, and debug information.\n- It updates statistics based on the search results, including match counts, file counts, and timing information.\n- It integrates document ranking and truncation based on specified options.\n- It contains detailed logic for retrieving branch indexes from file branch masks.\n\nOverall, the code performs the core search functionality within the indexing system, processing queries against the indexed data and returning relevant search results based on the given search options.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:48 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:47.822Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d49490015f77d37d69f00d7e179c12db
      _order: 0
      cache: {}
      request:
        bodySize: 7891
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-b9e618ca67b71661440a0f34f37987a1-46aaee6a1649b05e-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestSendByRepository(t *testing.T) {
                  	wantStats := zoekt.Stats{}
                  	wantStats.ShardsScanned = 1

                  	// n1, n2, n3 are the number of file matches for each of the 3 repositories in this
                  	// test.
                  	f := func(n1, n2, n3 uint8) bool {
                  		sr := createMockSearchResult(n1, n2, n3, wantStats)

                  		mock := &mockSender{}
                  		sendByRepository(sr, &zoekt.SearchOptions{}, mock)

                  		if diff := cmp.Diff(wantStats, mock.stats); diff != "" {
                  			t.Logf("-want,+got/n%s", diff)
                  			return false
                  		}

                  		nonZero := 0
                  		for _, l := range []uint8{n1, n2, n3} {
                  			if l > 0 {
                  				nonZero++
                  			}
                  		}
                  		if l := len(mock.files); l != nonZero {
                  			t.Logf("wanted results from %d repositores, got %d", nonZero, l)
                  			return false
                  		}

                  		gotTotal := 0
                  		for _, fs := range mock.files {
                  			gotTotal += len(fs)
                  		}
                  		wantTotal := int(n1) + int(n2) + int(n3)
                  		if gotTotal != wantTotal {
                  			t.Logf("wanted %d file matches, got %d", wantTotal, gotTotal)
                  			return false
                  		}

                  		for _, fs := range mock.files {
                  			if len(fs) == 0 {
                  				t.Logf("got search result with 0 file matches after split")
                  				return false
                  			}
                  		}
                  		return true
                  	}

                  	if err := quick.Check(f, nil); err != nil {
                  		t.Error(err)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file grpc/chunk/chunk.go:

                  ```go

                  func SendAll[T proto.Message](sendFunc func([]T) error, items ...T) error {
                  	c := New(sendFunc)

                  	err := c.Send(items...)
                  	if err != nil {
                  		return err
                  	}

                  	return c.Flush()
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: is my code base clean?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 78914
        content:
          mimeType: text/event-stream
          size: 78914
          text: >+
            event: completion

            data: {"completion":"Based on the provided context, the code snippet defines a function `SendAll` that takes a `sendFunc` function as a parameter along with a variable number of items of type `T` as input. The function creates a new `Chunk` instance, sends the items to the `Chunk` instance using the `send` method, and then flushes the chunk. It appears to be a utility function for sending a batch of items using the provided `sendFunc`. \n\nTo assess whether your codebase is clean, I would need to understand how this function is used in the codebase and examine other parts of the codebase for consistency, readability, adherence to best practices, proper error handling, etc. Could you provide me with additional context or specific criteria that you are concerned about in your codebase so I can give you a more detailed assessment?","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:54 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:53.723Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 5dd60cb04cbb60b076f653a2cfd8ba73
      _order: 0
      cache: {}
      request:
        bodySize: 10333
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-30e0a00a0fb333f5bdc43905396ae716-ec6893ebbe2119cd-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Can you look through the
                  files and identify the conflicting packages that may be
                  causing this?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 135642
        content:
          mimeType: text/event-stream
          size: 135642
          text: >+
            event: completion

            data: {"completion":"Based on the provided code, the `limit.go` file in the `zoekt` package contains functions related to sorting and truncating file matches based on search options in the Zoekt search engine. \n\n- The `SortAndTruncateFiles` function sorts and truncates the list of `FileMatch` items based on search options.\n- The `DisplayTruncator` function is a stateful function that enforces limits on document and match display by truncating and mutating before. It returns a subset of `FileMatch` items and a flag indicating if more items are available.\n- The `NewDisplayTruncator` function creates a `DisplayTruncator` based on search options.\n- The `limitMatches` function limits the number of matches in each `FileMatch`.\n- The `limitChunkMatches` and `limitLineMatches` functions limit the number of chunk matches and line matches within a `FileMatch`, respectively. \n\nTo identify conflicting packages that may be causing issues, one would need to analyze the import statements in the code to see if there are any conflicting package imports from external libraries or within the same project.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:10:58 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:10:57.405Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 9925dba2606c42ecdc22a28cdae36915
      _order: 0
      cache: {}
      request:
        bodySize: 10475
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-9d0c3e42a475c2048dd293699b5e2965-bdb11a3fa088b176-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file vscode/src/external-services.ts:
                  ```typescript

                  ```typescript
                          },
                      }
                  }
                  ``````
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Mind taking a second look at
                  the file? @limit.go"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 26326
        content:
          mimeType: text/event-stream
          size: 26326
          text: >+
            event: completion

            data: {"completion":"The limit.go file in the codebase contains functions related to sorting and truncating file matches based on search options in the Zoekt search engine. The SortAndTruncateFiles function sorts files and truncates them based on search options. The DisplayTruncator function enforces limits on document and match display counts and truncates the results accordingly. The file also contains functions for limiting the number of chunk matches and line matches in file matches.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:11:02 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:11:01.662Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 03174b39163b1894c3879864766688ad
      _order: 0
      cache: {}
      request:
        bodySize: 10281
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-688312aba76329c48bd300884daa52c2-9c6dd420d2617db0-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Why does this project use
                  the MIT license?"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 97218
        content:
          mimeType: text/event-stream
          size: 97218
          text: >+
            event: completion

            data: {"completion":"Based on the provided code context, the `limit.go` file in the `zoekt` package contains functions related to sorting and truncating file matches based on search options in the Zoekt search engine. The `SortAndTruncateFiles` function sorts and truncates file matches, while the `DisplayTruncator` function enforces document and match display limits by truncating and mutating before indicating if there are more limits to enforce. The `NewDisplayTruncator` function creates a display truncator with options limits. The `limitMatches`, `limitChunkMatches`, and `limitLineMatches` functions limit the number of chunk matches and line matches in file matches, respectively.\n\nWhile the provided code context does not directly mention the project's license, it is common for open-source projects like Zoekt to use the MIT license due to its permissive nature that allows users to freely use, modify, and distribute the software.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:11:04 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:11:03.648Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 0acf01d70ee862b3e746d10e37fb6c37
      _order: 0
      cache: {}
      request:
        bodySize: 10286
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-77be466e574b4a535414a4e94a737e92-8da8811cfad65954-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: See zoekt repo find location
                  of tensor function"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 96845
        content:
          mimeType: text/event-stream
          size: 96845
          text: >+
            event: completion

            data: {"completion":"Based on the provided code, the `zoekt` package contains functions for sorting and truncating file matches based on search options. The `SortAndTruncateFiles` function sorts and truncates file matches, while the `DisplayTruncator` function enforces display limits for documents and matches. The `NewDisplayTruncator` function creates a display truncator based on search options, enforcing limits for document and match display counts.\n\nThe code contains functions to limit the number of chunk matches and line matches in file matches. The `limitChunkMatches` function limits the number of chunk matches in a file match by truncating content and symbol information if necessary. The `limitLineMatches` function limits the number of line matches in a file match.\n\nTo find the location of the `tensor` function in the `zoekt` repository, additional context about the structure and content of the repository itself would be needed.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:11:08 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:11:07.398Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: a00c7c55b86473805a812ab2e531a161
      _order: 0
      cache: {}
      request:
        bodySize: 17620
        cookies: []
        headers:
          - name: content-type
            value: application/json
          - name: accept-encoding
            value: gzip;q=0
          - name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - name: user-agent
            value: chat-response-quality / v1
          - name: traceparent
            value: 00-eac0ae801201542d02f83ff4a40c8f69-219cd844c8f1d982-01
          - name: connection
            value: keep-alive
          - name: host
            value: sourcegraph.com
        headersSize: 417
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json
          params: []
          textJSON:
            maxTokensToSample: 4000
            messages:
              - speaker: human
                text: You are Cody, an AI coding assistant from Sourcegraph.
              - speaker: assistant
                text: I am Cody, an AI coding assistant from Sourcegraph.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  type rankSearcher struct {
                  	rank uint16
                  	repo *zoekt.Repository
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestWordBoundaryRanking(t *testing.T) {
                  	cases := []struct {
                  		name              string
                  		testShardedSearch func(t *testing.T, q query.Q, ib *zoekt.IndexBuilder, useDocumentRanks bool) []zoekt.FileMatch
                  	}{
                  		{"Search", testShardedSearch},
                  		{"StreamSearch", testShardedStreamSearch},
                  	}

                  	b := testIndexBuilder(t, nil,
                  		zoekt.Document{Name: "f1", Content: []byte("xbytex xbytex")},
                  		zoekt.Document{Name: "f2", Content: []byte("xbytex/nbytex/nbyte bla")},
                  		// -----------------------------------------0123456 789012 34567890
                  		zoekt.Document{Name: "f3", Content: []byte("xbytex ybytex")})

                  	for _, tt := range cases {
                  		for _, useDocumentRanks := range []bool{false, true} {
                  			t.Run(tt.name, func(t *testing.T) {
                  				files := tt.testShardedSearch(t, &query.Substring{Pattern: "byte"}, b, useDocumentRanks)

                  				if len(files) != 3 {
                  					t.Fatalf("got %#v, want 3 files", files)
                  				}

                  				file0 := files[0]
                  				if file0.FileName != "f2" || len(file0.LineMatches) != 3 {
                  					t.Fatalf("got file %s, num matches %d (%#v), want 3 matches in file f2", file0.FileName, len(file0.LineMatches), file0)
                  				}

                  				if file0.LineMatches[0].LineFragments[0].Offset != 13 {
                  					t.Fatalf("got first match %#v, want full word match", files[0].LineMatches[0])
                  				}
                  				if file0.LineMatches[1].LineFragments[0].Offset != 7 {
                  					t.Fatalf("got second match %#v, want partial word match", files[0].LineMatches[1])
                  				}
                  			})
                  		}
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_Ranking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f2", Content: []byte("foo bas")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f1", Content: []byte("foo bar bas")})

                  	want := []string{
                  		"super-star",
                  		"moderately-popular",
                  		"weekend-project",
                  		"weekend-project-2",
                  	}

                  	var have []string
                  	for _, s := range ss.getLoaded().shards {
                  		for _, r := range s.repos {
                  			have = append(have, r.Name)
                  		}
                  	}

                  	if !reflect.DeepEqual(want, have) {
                  		t.Fatalf("/nwant: %s/nhave: %s", want, have)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func TestShardedSearcher_DocumentRanking(t *testing.T) {
                  	ss := newShardedSearcher(1)

                  	var nextShardNum int
                  	addShard := func(repo string, priority float64, docs ...zoekt.Document) {
                  		r := &zoekt.Repository{ID: hash(repo), Name: repo}
                  		r.RawConfig = map[string]string{
                  			"public":   "1",
                  			"priority": strconv.FormatFloat(priority, 'f', 2, 64),
                  		}
                  		b := testIndexBuilder(t, r, docs...)
                  		shard := searcherForTest(t, b)
                  		ss.replace(map[string]zoekt.Searcher{
                  			fmt.Sprintf("key-%d", nextShardNum): shard,
                  		})
                  		nextShardNum++
                  	}

                  	addShard("weekend-project", 20, zoekt.Document{Name: "f1", Content: []byte("foobar")})
                  	addShard("moderately-popular", 500, zoekt.Document{Name: "f2", Content: []byte("foobaz")})
                  	addShard("weekend-project-2", 20, zoekt.Document{Name: "f3", Content: []byte("foo bar")})
                  	addShard("super-star", 5000, zoekt.Document{Name: "f4", Content: []byte("foo baz")},
                  		zoekt.Document{Name: "f5", Content: []byte("fooooo")})

                  	// Run a stream search and gather the results
                  	var results []*zoekt.SearchResult
                  	opts := &zoekt.SearchOptions{
                  		UseDocumentRanks: true,
                  		FlushWallTime:    100 * time.Millisecond,
                  	}

                  	err := ss.StreamSearch(context.Background(), &query.Substring{Pattern: "foo"}, opts,
                  		zoekt.SenderFunc(func(event *zoekt.SearchResult) {
                  			results = append(results, event)
                  		}))
                  	if err != nil {
                  		t.Fatal(err)
                  	}

                  	// There should always be two stream results, first progress-only, then the file results
                  	if len(results) != 2 {
                  		t.Fatalf("expected 2 streamed results, but got %d", len(results))
                  	}

                  	// The ranking should be determined by whether it's an exact word match,
                  	// followed by repository priority
                  	want := []string{"f4", "f3", "f5", "f2", "f1"}

                  	files := results[1].Files
                  	got := make([]string, len(files))
                  	for i := 0; i < len(files); i++ {
                  		got[i] = files[i].FileName
                  	}

                  	if !reflect.DeepEqual(got, want) {
                  		t.Errorf("got %v, want %v", got, want)
                  	}
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) String() string {
                  	return ""
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Search(ctx context.Context, q query.Q, opts *zoekt.SearchOptions) (*zoekt.SearchResult, error) {
                  	select {
                  	case <-ctx.Done():
                  		return &zoekt.SearchResult{}, nil
                  	default:
                  	}

                  	// Ugly, but without sleep it's too fast, and we can't
                  	// simulate the cutoff.
                  	time.Sleep(time.Millisecond)
                  	return &zoekt.SearchResult{
                  		Files: []zoekt.FileMatch{
                  			{
                  				FileName: fmt.Sprintf("f%d", s.rank),
                  				Score:    float64(s.rank),
                  			},
                  		},
                  		Stats: zoekt.Stats{
                  			MatchCount: 1,
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) Repository() *zoekt.Repository { return s.repo }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file shards/shards_test.go:

                  ```go

                  func (s *rankSearcher) List(ctx context.Context, q query.Q, opts *zoekt.ListOptions) (*zoekt.RepoList, error) {
                  	r := zoekt.Repository{}
                  	if s.repo != nil {
                  		r = *s.repo
                  	}
                  	r.Rank = s.rank
                  	return &zoekt.RepoList{
                  		Repos: []*zoekt.RepoListEntry{
                  			{Repository: r},
                  		},
                  	}, nil
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: |-
                  Codebase context from file shards/shards_test.go:
                  ```go
                  func (s *rankSearcher) Close() {
                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file README.md:

                  ```markdown

                      "Zoekt, en gij zult spinazie eten" - Jan Eertink

                      ("seek, and ye shall eat spinach" - My primary school teacher)

                  This is a fast text search engine, intended for use with source

                  code. (Pronunciation: roughly as you would pronounce "zooked" in English)


                  **Note:** This is a [Sourcegraph](https://github.com/sourcegraph/zoekt) fork

                  of [github.com/google/zoekt](https://github.com/google/zoekt). It is now the

                  main maintained source of Zoekt.


                  # INSTRUCTIONS


                  ## Downloading

                      go get github.com/sourcegraph/zoekt/

                  ## Indexing


                  ### Directory

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-index
                      $GOPATH/bin/zoekt-index .

                  ### Git repository

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-git-index
                      $GOPATH/bin/zoekt-git-index -branches master,stable-1.4 -prefix origin/ .

                  ### Repo repositories

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-{repo-index,mirror-gitiles}
                      zoekt-mirror-gitiles -dest ~/repos/ https://gfiber.googlesource.com
                      zoekt-repo-index /n        -name gfiber /n        -base_url https://gfiber.googlesource.com/ /n        -manifest_repo ~/repos/gfiber.googlesource.com/manifests.git /n        -repo_cache ~/repos /n        -manifest_rev_prefix=refs/heads/ --rev_prefix= /n        master:default_unrestricted.xml

                  ## Searching


                  ### Web interface

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-webserver
                      $GOPATH/bin/zoekt-webserver -listen :6070

                  ### JSON API


                  You can retrieve search results as JSON by sending a GET request to zoekt-webserver.

                      curl --get /n        --url "http://localhost:6070/search" /n        --data-urlencode "q=ngram f:READ" /n        --data-urlencode "num=50" /n        --data-urlencode "format=json"

                  The response data is a JSON object. You can refer to [web.ApiSearchResult](https://sourcegraph.com/github.com/sourcegraph/zoekt@6b1df4f8a3d7b34f13ba0cafd8e1a9b3fc728cf0/-/blob/web/api.go?L23:6&subtree=true) to learn about the structure of the object.


                  ### CLI

                      go install github.com/sourcegraph/zoekt/cmd/zoekt
                      $GOPATH/bin/zoekt 'ngram f:READ'

                  ## Installation

                  A more organized installation on a Linux server should use a systemd unit file,

                  eg.

                      [Unit]
                      Description=zoekt webserver

                      [Service]
                      ExecStart=/zoekt/bin/zoekt-webserver -index /zoekt/index -listen :443  --ssl_cert /zoekt/etc/cert.pem   --ssl_key /zoekt/etc/key.pem
                      Restart=always

                      [Install]
                      WantedBy=default.target


                  # SEARCH SERVICE


                  Zoekt comes with a small service management program:

                      go install github.com/sourcegraph/zoekt/cmd/zoekt-indexserver

                      cat << EOF > config.json
                      [{"GithubUser": "username"},
                       {"GithubOrg": "org"},
                       {"GitilesURL": "https://gerrit.googlesource.com", "Name": "zoekt" }
                      ]
                      EOF

                      $GOPATH/bin/zoekt-indexserver -mirror_config config.json

                  This will mirror all repos under 'github.com/username', 'github.com/org', as

                  well as the 'zoekt' repository. It will index the repositories.


                  It takes care of fetching and indexing new data and cleaning up logfiles.


                  The webserver can be started from a standard service management framework, such

                  as systemd.



                  # SYMBOL SEARCH


                  It is recommended to install [Universal

                  ctags](https://github.com/universal-ctags/ctags) to improve

                  ranking. See [here](doc/ctags.md) for more information.



                  # ACKNOWLEDGEMENTS


                  Thanks to Han-Wen Nienhuys for creating Zoekt. Thanks to Alexander Neubeck for

                  coming up with this idea, and helping Han-Wen Nienhuys flesh it out.



                  # FORK DETAILS


                  Originally this fork contained some changes that do not make sense to upstream

                  and or have not yet been upstreamed. However, this is now the defacto source

                  for Zoekt. This section will remain for historical reasons and contains

                  outdated information. It can be removed once the dust settles on moving from

                  google/zoekt to sourcegraph/zoekt. Differences:


                  - [zoekt-sourcegraph-indexserver](cmd/zoekt-sourcegraph-indexserver/main.go)
                    is a Sourcegraph specific command which indexes all enabled repositories on
                    Sourcegraph, as well as keeping the indexes up to date.
                  - We have exposed the API via
                    [keegancsmith/rpc](https://github.com/keegancsmith/rpc) (a fork of `net/rpc`
                    which supports cancellation).
                  - Query primitive `BranchesRepos` to efficiently specify a set of repositories to
                    search.
                  - Allow empty shard directories on startup. Needed when starting a fresh
                    instance which hasn't indexed anything yet.
                  - We can return symbol/ctag data in results. Additionally we can run symbol regex queries.

                  - We search shards in order of repo name and ignore shard ranking.

                  - Other minor changes.


                  Assuming you have the gerrit upstream configured, a useful way to see what we

                  changed is:


                  ``` shellsession

                  $ git diff gerrit/master -- ':(exclude)vendor/' ':(exclude)Gopkg*'

                  ```


                  # DISCLAIMER


                  This is not an official Google product

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: >-
                  Codebase context from file limit.go:

                  ```go

                  package zoekt


                  import "log"


                  // SortAndTruncateFiles is a convenience around SortFiles and

                  // DisplayTruncator. Given an aggregated files it will sort and then truncate

                  // based on the search options.

                  func SortAndTruncateFiles(files []FileMatch, opts *SearchOptions) []FileMatch {
                  	SortFiles(files)
                  	truncator, _ := NewDisplayTruncator(opts)
                  	files, _ = truncator(files)
                  	return files
                  }


                  // DisplayTruncator is a stateful function which enforces Document and Match

                  // display limits by truncating and mutating before. hasMore is true until the

                  // limits are exhausted. Once hasMore is false each subsequent call will

                  // return an empty after and hasMore false.

                  type DisplayTruncator func(before []FileMatch) (after []FileMatch, hasMore bool)


                  // NewDisplayTruncator will return a DisplayTruncator which enforces the limits in

                  // opts. If there are no limits to enforce, hasLimits is false and there is no

                  // need to call DisplayTruncator.

                  func NewDisplayTruncator(opts *SearchOptions) (_ DisplayTruncator, hasLimits bool) {
                  	docLimit := opts.MaxDocDisplayCount
                  	docLimited := docLimit > 0

                  	matchLimit := opts.MaxMatchDisplayCount
                  	matchLimited := matchLimit > 0

                  	done := false

                  	if !docLimited && !matchLimited {
                  		return func(fm []FileMatch) ([]FileMatch, bool) {
                  			return fm, true
                  		}, false
                  	}

                  	return func(fm []FileMatch) ([]FileMatch, bool) {
                  		if done {
                  			return nil, false
                  		}

                  		if docLimited {
                  			if len(fm) >= docLimit {
                  				done = true
                  				fm = fm[:docLimit]
                  			}
                  			docLimit -= len(fm)
                  		}

                  		if matchLimited {
                  			fm, matchLimit = limitMatches(fm, matchLimit, opts.ChunkMatches)
                  			if matchLimit <= 0 {
                  				done = true
                  			}
                  		}

                  		return fm, !done
                  	}, true
                  }


                  func limitMatches(files []FileMatch, limit int, chunkMatches bool) ([]FileMatch, int) {
                  	var limiter func(file *FileMatch, limit int) int
                  	if chunkMatches {
                  		limiter = limitChunkMatches
                  	} else {
                  		limiter = limitLineMatches
                  	}
                  	for i := range files {
                  		limit = limiter(&files[i], limit)
                  		if limit <= 0 {
                  			return files[:i+1], 0
                  		}
                  	}
                  	return files, limit
                  }


                  // Limit the number of ChunkMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitChunkMatches(file *FileMatch, limit int) int {
                  	for i := range file.ChunkMatches {
                  		cm := &file.ChunkMatches[i]
                  		if len(cm.Ranges) > limit {
                  			// We potentially need to effect the limit upon 3 different fields:
                  			// Ranges, SymbolInfo, and Content.

                  			// Content is the most complicated: we need to remove the last N
                  			// lines from it, where N is the difference between the line number
                  			// of the end of the old last Range and that of the new last Range.
                  			// This calculation is correct in the presence of both context lines
                  			// and multiline Ranges, taking into account that Content never has
                  			// a trailing newline.
                  			n := cm.Ranges[len(cm.Ranges)-1].End.LineNumber - cm.Ranges[limit-1].End.LineNumber
                  			if n > 0 {
                  				for b := len(cm.Content) - 1; b >= 0; b-- {
                  					if cm.Content[b] == '/n' {
                  						n -= 1
                  					}
                  					if n == 0 {
                  						cm.Content = cm.Content[:b]
                  						break
                  					}
                  				}
                  				if n > 0 {
                  					// Should be impossible.
                  					log.Panicf("Failed to find enough newlines when truncating Content, %d left over, %d ranges", n, len(cm.Ranges))
                  				}
                  			}

                  			cm.Ranges = cm.Ranges[:limit]
                  			if cm.SymbolInfo != nil {
                  				// When non-nil, SymbolInfo is specified to have the same length
                  				// as Ranges.
                  				cm.SymbolInfo = cm.SymbolInfo[:limit]
                  			}
                  		}
                  		if len(cm.Ranges) == limit {
                  			file.ChunkMatches = file.ChunkMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(cm.Ranges)
                  	}
                  	return limit
                  }


                  // Limit the number of LineMatches in the given FileMatch, returning the

                  // remaining limit, if any.

                  func limitLineMatches(file *FileMatch, limit int) int {
                  	for i := range file.LineMatches {
                  		lm := &file.LineMatches[i]
                  		if len(lm.LineFragments) > limit {
                  			lm.LineFragments = lm.LineFragments[:limit]
                  		}
                  		if len(lm.LineFragments) == limit {
                  			file.LineMatches = file.LineMatches[:i+1]
                  			limit = 0
                  			break
                  		}
                  		limit -= len(lm.LineFragments)
                  	}
                  	return limit
                  }

                  ```
              - speaker: assistant
                text: Ok.
              - speaker: human
                text: "The provided context are the code you need and have access to. Do not
                  make any assumptions. Give me a high-level overview based on
                  the given context first before asking for additional context
                  to answer the following question: Explain the logic in
                  src/agent.go, particularly how agents interact with ranking"
              - speaker: assistant
            model: openai/gpt-3.5-turbo
            temperature: 0
            topK: -1
            topP: -1
        queryString:
          - name: client-name
            value: chat-response-quality
          - name: client-version
            value: v1
        url: https://sourcegraph.com/.api/completions/stream?client-name=chat-response-quality&client-version=v1
      response:
        bodySize: 56345
        content:
          mimeType: text/event-stream
          size: 56345
          text: >+
            event: completion

            data: {"completion":"The code provided in limit.go contains logic related to sorting and truncating search results based on specified limits in the SearchOptions. The SortAndTruncateFiles function sorts the files and then truncates them based on document and match display limits. The NewDisplayTruncator function creates a DisplayTruncator function that enforces the limits set in the SearchOptions. The limitMatches, limitChunkMatches, and limitLineMatches functions handle limiting the number of matches and fragments in the search results.\n\nTo provide an overview of how agents interact with ranking in src/agent.go, I would need to see the code in src/agent.go that specifically relates to this interaction.","stopReason":"stop"}


            event: done

            data: {}

        cookies: []
        headers:
          - name: date
            value: Mon, 10 Jun 2024 23:11:13 GMT
          - name: content-type
            value: text/event-stream
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
        headersSize: 1299
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-10T23:11:12.557Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 12581f1c735a04aeb88af7a54cd007b2
      _order: 0
      cache: {}
      request:
        bodySize: 217
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "217"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 340
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CodyConfigFeaturesResponse {
                  site {
                      codyConfigFeatures {
                          chat
                          autoComplete
                          commands
                          attribution
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CodyConfigFeaturesResponse
            value: null
        url: https://sourcegraph.com/.api/graphql?CodyConfigFeaturesResponse
      response:
        bodySize: 155
        content:
          encoding: base64
          mimeType: application/json
          size: 155
          text: "[\"H4sIAAAAAAAAAzyLwQqA\",\"IBAF/2XPfYFXof/YdC0h3dDnIcR/Dws6DQwznTyDyXSq\
            ETLp1N9Wc4j7KoxWpL72YJBBabIQN6jVdJ0yj885TYmzr38DlLg1RM1kAp9VxhjjAQA\
            A//8DAIfOLkJuAAAA\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.152Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 09b59ac55ce3c40d6f9ab8c79846a2c6
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 244
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query ContextFilters {
                  site {
                      codyContextFilters(version: V1) {
                          raw
                      }
                  }
              }
            variables: {}
        queryString:
          - name: ContextFilters
            value: null
        url: https://sourcegraph.com/.api/graphql?ContextFilters
      response:
        bodySize: 107
        content:
          encoding: base64
          mimeType: application/json
          size: 107
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqzixJBdHJ+SmVzvl5JakVJW6ZOSWpRcUg0aLE\
            ciWrvNKcnNra2loAAAAA//8DADYuyGU1AAAA\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.372Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 9d0e130a38eaf9d3cd8d9512afc14b87
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 328
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query ContextFilters {
                  site {
                      codyContextFilters(version: V1) {
                          raw
                      }
                  }
              }
            variables: {}
        queryString:
          - name: ContextFilters
            value: null
        url: https://sourcegraph.com/.api/graphql?ContextFilters
      response:
        bodySize: 104
        content:
          encoding: base64
          mimeType: application/json
          size: 104
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdHJ+SmVzvl5JakVJW6ZOSWpRcUg0aLEciWrv\
            NKcnNra2loAAAAA//8DADYuyGU1AAAA\"]"
          textDecoded:
            data:
              site:
                codyContextFilters:
                  raw: null
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.271Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 460fa85fef77ffa15bb82fa3a88049a3
      _order: 0
      cache: {}
      request:
        bodySize: 318
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "318"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 345
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmConfiguration {
                  site {
                      codyLLMConfiguration {
                          chatModel
                          chatModelMaxTokens
                          fastChatModel
                          fastChatModelMaxTokens
                          completionModel
                          completionModelMaxTokens
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmConfiguration
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmConfiguration
      response:
        bodySize: 248
        content:
          encoding: base64
          mimeType: application/json
          size: 248
          text: "[\"H4sIAAAAAAAAA3zOwQqCQBAG4HeZs+K0BqFXr3rrBYbdMRdtJ3ZXKmTfPSyiSOg08PP/H\
            7OAoUhQLxBs5PVqMfe27RpxvT3NnqIV98wHip0YnqAGcnHwcrG60BPNhvMyD+Icx1yh\
            2qNSFWSfQUe3o4zsAtQ7hYgZ9BRi898byI7ziyvxAD+bLanlfJl4ffaN9tbzVfwYihD\
            JazHsYdP7gipETCmlBwAAAP//AwBQeP+1EwEAAA==\"]"
          textDecoded:
            data:
              site:
                codyLLMConfiguration:
                  chatModel: anthropic/claude-3-sonnet-20240229
                  chatModelMaxTokens: 12000
                  completionModel: fireworks/starcoder
                  completionModelMaxTokens: 9000
                  fastChatModel: anthropic/claude-3-haiku-20240307
                  fastChatModelMaxTokens: 12000
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.492Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 7edf90ea650cb304fe26f9d57bd79477
      _order: 0
      cache: {}
      request:
        bodySize: 165
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "165"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 345
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmConfiguration {
                  site {
                      codyLLMConfiguration {
                          smartContextWindow
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmConfiguration
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmConfiguration
      response:
        bodySize: 142
        content:
          encoding: base64
          mimeType: application/json
          size: 142
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqzixJBdHJ+SmVPj6+zvl5aZnppUWJJZn5eWD5\
            3MSiEuf8vJLUipLwzLyU/HIlK6XUvMSknNQUpdra2loAAAAA//8=\",\"AwDoCDSlSw\
            AAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.513Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: d890a82b3cc2fe4d514f289e8fe6d158
      _order: 0
      cache: {}
      request:
        bodySize: 150
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "150"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 340
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentSiteCodyLlmProvider {
                  site {
                      codyLLMConfiguration {
                          provider
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentSiteCodyLlmProvider
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentSiteCodyLlmProvider
      response:
        bodySize: 128
        content:
          encoding: base64
          mimeType: application/json
          size: 128
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdHJ+SmVPj6+zvl5aZnppUWJJZn5eSDxgqL8s\
            syU1CIlK6Xi/NKi5NT0osSCDKXa2tpaAAAAAP//AwAfFAXARQAAAA==\"]"
          textDecoded:
            data:
              site:
                codyLLMConfiguration:
                  provider: sourcegraph
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.493Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 7f3ff10a39505669cb5d36145df5fa95
      _order: 0
      cache: {}
      request:
        bodySize: 227
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "227"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 325
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentUser {
                  currentUser {
                      id
                      hasVerifiedEmail
                      displayName
                      username
                      avatarURL
                      primaryEmail {
                          email
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentUser
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentUser
      response:
        bodySize: 355
        content:
          encoding: base64
          mimeType: application/json
          size: 355
          text: "[\"H4sIAAAAAAAAA2RPy04=\",\"g0AU/Ze7hoJptDBJE21tXVSJj5TU5WW4hQGGwXlUKeHf\
            G1ITF+7OyTk5jwFytAhsAO60ptbuDemJihwYpIek4ZU6J49vNy8VX4IHJZqUtDgKyjc\
            SRQPMakce5MJ0DfYJSgIGH8ppToXGrlwp68dhGIIHzpBurwbzZ8iUjWv/2H5LBx7gCS\
            3q/fszMCit7QwLgqaczwqlioamBK5aS62dcSUDDB7WRaT4botf2Se5VZ1Vt/l2c/6Js\
            kMa4ULMTZrt1slrungKXX+qlya+8zl40GkhUfe/JwagK/i37L6YhKkNxnEcLwAAAP//\
            AwC04S4cMQEAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.534Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 84b962509b12000d0eef7c8a8fa655f3
      _order: 0
      cache: {}
      request:
        bodySize: 268
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "268"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 341
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query CurrentUserCodySubscription {
                  currentUser {
                      codySubscription {
                          status
                          plan
                          applyProRateLimits
                          currentPeriodStartAt
                          currentPeriodEndAt
                      }
                  }
              }
            variables: {}
        queryString:
          - name: CurrentUserCodySubscription
            value: null
        url: https://sourcegraph.com/.api/graphql?CurrentUserCodySubscription
      response:
        bodySize: 228
        content:
          encoding: base64
          mimeType: application/json
          size: 228
          text: "[\"H4sIAAAAAAAAA1zMsQrCMBSF4Xc5c4U2VodsRToIgqWtDm6xyRCoSbi5GUrJu4uCoI7n5\
            +Os0IoV5IopERnHl2joPb1ehnSPE9nA1rtXi6w4RUg0h/F4bVEgzMpBouvPKKBCmJeO\
            fK/YnOzDcoRkSqb4fHeGrNcDK+KGISFKUW/K3aaqRyFkVcmtuOFPt05/2f2vzTnnJwA\
            AAP//AwBSFe7+wgAAAA==\"]"
          textDecoded:
            data:
              currentUser:
                codySubscription:
                  applyProRateLimits: true
                  currentPeriodEndAt: 2024-06-14T22:11:32Z
                  currentPeriodStartAt: 2024-05-14T22:11:32Z
                  plan: PRO
                  status: ACTIVE
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.818Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: ea3ddfc16f0b52a2180a9bd21c87dc08
      _order: 0
      cache: {}
      request:
        bodySize: 144
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "144"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 240
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query Repository($name: String!) {
              	repository(name: $name) {
              		id
              	}
              }
            variables:
              name: github.com/sourcegraph/cody
        queryString:
          - name: Repository
            value: null
        url: https://sourcegraph.com/.api/graphql?Repository
      response:
        bodySize: 123
        content:
          encoding: base64
          mimeType: application/json
          size: 123
          text: "[\"H4sIAAAAAAAAA6pWSkks\",\"SVSyqlYqSi3IL84syS+qBPEyU5SslEJzw8qTjP0KUtwt\
            K1ND8o18Q3wr/UJ8K/0dbW2VamtrAQAAAP//AwDHAhygPQAAAA==\"]"
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.035Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 8d297306aeea324b87ef494954016fba
      _order: 0
      cache: {}
      request:
        bodySize: 164
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "164"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 248
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteIdentification {
              	site {
              		siteID
              		productSubscription {
              			license {
              				hashedKey
              			}
              		}
              	}
              }
            variables: {}
        queryString:
          - name: SiteIdentification
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteIdentification
      response:
        bodySize: 212
        content:
          encoding: base64
          mimeType: application/json
          size: 212
          text: "[\"H4sIAAAAAAAAAzTLsQ6CMBCA4Xe52YX2rgVmF+PI4HztXaWJAdKWwRDf3WDiv/zTd4BwY\
            xgPqLnp/7crjDCte4n6LLzNDw1wga2sssc27aHGkreW1+UErxx1qT87c51V7vqGEYbo\
            u9AZm/okmgxi70QZlZzzqNEaJOMNReocCdkgiCk4j4btwJwIPmdfAAAA//8DABj2u36\
            gAAAA\"]"
          textDecoded:
            data:
              site:
                productSubscription:
                  license:
                    hashedKey: 9c71b123f8fdef24486dea4e56674ec32452725c5165d53bd44fb6742a39aaf5
                siteID: SourcegraphWeb
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:55.993Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: b584bf96a3d88ab96114e5cbea1e4bca
      _order: 0
      cache: {}
      request:
        bodySize: 164
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "164"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 332
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteIdentification {
              	site {
              		siteID
              		productSubscription {
              			license {
              				hashedKey
              			}
              		}
              	}
              }
            variables: {}
        queryString:
          - name: SiteIdentification
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteIdentification
      response:
        bodySize: 212
        content:
          encoding: base64
          mimeType: application/json
          size: 212
          text: "[\"H4sIAAAAAAAAAzTLsQ6CMBCA4Xe52YX2rgVmF+PI4HztXaWJAdKWwRDf3WDiv/zTd4BwY\
            xgPqLnp/7crjDCte4n6LLzNDw1wga2sssc27aHGkreW1+UErxx1qT87c51V7vqGEYbo\
            u9AZm/okmgxi70QZlZzzqNEaJOMNReocCdkgiCk4j4btwJwIPmdfAAAA//8DABj2u36\
            gAAAA\"]"
          textDecoded:
            data:
              site:
                productSubscription:
                  license:
                    hashedKey: 9c71b123f8fdef24486dea4e56674ec32452725c5165d53bd44fb6742a39aaf5
                siteID: SourcegraphWeb
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:57 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:57.151Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 203c1896021c3a09dfe619120ea1b725
      _order: 0
      cache: {}
      request:
        bodySize: 101
        cookies: []
        headers:
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "101"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 248
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteProductVersion {
                  site {
                      productVersion
                  }
              }
            variables: {}
        queryString:
          - name: SiteProductVersion
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteProductVersion
      response:
        bodySize: 136
        content:
          encoding: base64
          mimeType: application/json
          size: 136
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdEFRfkppcklYalFxZn5eUpWSkbm5obmJvFGB\
            kYmugZmugam8aZ6JrpJpuYmxgYG5oYmpqlKtbW1AAAAAP//AwAHA5EmSQAAAA==\"]"
          textDecoded:
            data:
              site:
                productVersion: 277174_2024-06-05_5.4-b5743007145e
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.260Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
    - _id: 2aa42833ae189b030c5bc322f1d27b0c
      _order: 0
      cache: {}
      request:
        bodySize: 101
        cookies: []
        headers:
          - _fromType: array
            name: authorization
            value: token
              REDACTED_d5e0f0a37c9821e856b923fe14e67a605e3f6c0a517d5a4f46a4e35943ee0f6d
          - _fromType: array
            name: content-type
            value: application/json; charset=utf-8
          - _fromType: array
            name: user-agent
            value: chat-response-quality / v1
          - _fromType: array
            name: accept
            value: "*/*"
          - _fromType: array
            name: content-length
            value: "101"
          - _fromType: array
            name: accept-encoding
            value: gzip,deflate
          - name: host
            value: sourcegraph.com
        headersSize: 332
        httpVersion: HTTP/1.1
        method: POST
        postData:
          mimeType: application/json; charset=utf-8
          params: []
          textJSON:
            query: |-
              
              query SiteProductVersion {
                  site {
                      productVersion
                  }
              }
            variables: {}
        queryString:
          - name: SiteProductVersion
            value: null
        url: https://sourcegraph.com/.api/graphql?SiteProductVersion
      response:
        bodySize: 136
        content:
          encoding: base64
          mimeType: application/json
          size: 136
          text: "[\"H4sIAAAAAAAAA6pWSkksSVSyqlYqzixJBdEFRfkppcklYalFxZn5eUpWSkbm5obmJvFGB\
            kYmugZmugam8aZ6JrpJpuYmxgYG5oYmpqlKtbW1AAAAAP//AwAHA5EmSQAAAA==\"]"
          textDecoded:
            data:
              site:
                productVersion: 277174_2024-06-05_5.4-b5743007145e
        cookies: []
        headers:
          - name: date
            value: Wed, 05 Jun 2024 18:44:56 GMT
          - name: content-type
            value: application/json
          - name: transfer-encoding
            value: chunked
          - name: connection
            value: keep-alive
          - name: access-control-allow-credentials
            value: "true"
          - name: access-control-allow-origin
            value: ""
          - name: cache-control
            value: no-cache, max-age=0
          - name: vary
            value: Cookie,Accept-Encoding,Authorization,Cookie, Authorization,
              X-Requested-With,Cookie
          - name: x-content-type-options
            value: nosniff
          - name: x-frame-options
            value: DENY
          - name: x-xss-protection
            value: 1; mode=block
          - name: strict-transport-security
            value: max-age=31536000; includeSubDomains; preload
          - name: content-encoding
            value: gzip
        headersSize: 1333
        httpVersion: HTTP/1.1
        redirectURL: ""
        status: 200
        statusText: OK
      startedDateTime: 2024-06-05T18:44:56.490Z
      time: 0
      timings:
        blocked: -1
        connect: -1
        dns: -1
        receive: 0
        send: 0
        ssl: -1
        wait: 0
  pages: []
  version: "1.2"
